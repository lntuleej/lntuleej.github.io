<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="目标检测,深度学习,关系网络," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="Relation Networks for Object Detection 论文翻译">
<meta name="keywords" content="目标检测,深度学习,关系网络">
<meta property="og:type" content="article">
<meta property="og:title" content="Relation Networks for Object Detection">
<meta property="og:url" content="http://ailee.me/2018/06/18/Relation Networks for Object Detection/index.html">
<meta property="og:site_name" content="AILEE">
<meta property="og:description" content="Relation Networks for Object Detection 论文翻译">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://blog.adminlee.com/18-6-19/24010109.jpg">
<meta property="og:image" content="http://blog.adminlee.com/18-6-27/6106452.jpg">
<meta property="og:image" content="http://blog.adminlee.com/18-7-19/76449911.jpg">
<meta property="og:image" content="http://blog.adminlee.com/18-7-22/26962631.jpg">
<meta property="og:image" content="http://blog.adminlee.com/18-7-22/11324410.jpg">
<meta property="og:image" content="http://blog.adminlee.com/18-7-22/12870677.jpg">
<meta property="og:image" content="http://blog.adminlee.com/18-7-22/50028070.jpg">
<meta property="og:image" content="http://blog.adminlee.com/18-7-22/33110178.jpg">
<meta property="og:updated_time" content="2019-04-01T15:55:29.358Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Relation Networks for Object Detection">
<meta name="twitter:description" content="Relation Networks for Object Detection 论文翻译">
<meta name="twitter:image" content="http://blog.adminlee.com/18-6-19/24010109.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"hide","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://ailee.me/2018/06/18/Relation Networks for Object Detection/"/>





  <title>Relation Networks for Object Detection | AILEE</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?1e9e36dfff2baf430f723f50d54d376e";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->










</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AILEE</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://ailee.me/2018/06/18/Relation Networks for Object Detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ailee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AILEE">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Relation Networks for Object Detection</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-18T09:42:56+08:00">
                2018-06-18
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Update</span>
              
              <time title="Update" itemprop="dateModified" datetime="2019-04-01T23:55:29+08:00">
                2019-04-01
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">Category</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/06/18/Relation Networks for Object Detection/" class="leancloud_visitors" data-flag-title="Relation Networks for Object Detection">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">View </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">&nbsp;&nbsp;|&nbsp;&nbsp;
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words</span>
                
                <span title="Words">
                  9,146
                </span>
              

              

              
            </div>
          

          
              <div class="post-description">
                  Relation Networks for Object Detection 论文翻译
              </div>
          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>论文原文：<a href="https://arxiv.org/abs/1711.11575" target="_blank" rel="external">Relation Networks for Object Detection</a><br>论文作者：Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei</p>
</blockquote>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>　　多年来，尽管人们相信在对象之间进行关系建模将有助于对象识别，但是，在深度学习时代，还没有证据表明这个观点是可行的。所有最先进的目标检测系统仍旧依赖于独立的识别对象实例，而没有在学习的过程中使用对象之间的关系。<br>　　这篇文章提出了一种对象关系模型。它通过对象之间的外表特征和几何特征之间的相互作用，同时处理一组对象，从而对它们之间的关系进行建模。这是一个轻量级模型。它不需要额外的监督，并且很容易嵌入到已存在的网络中。在现代目标检测系统中，它对改善对象识别和清除重复步骤有效。在基于CNN的检测器中证明了关系网络建模的有效性。它产生了第一个完全端到端的对象检测器。</p>
<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><p>　　近年来，使用深度卷积神经网络在对象检测领域取得了显著的进步。最先进的对象检测方法大多是基于区域范例的，这是在开创性工作的R-CNN中建立的。给出一组稀疏的区域建议，在每一个区域建议上独立的进行对象分类和边界框回归。应用一种启发式的手工后处理步骤——非最大抑制(NMS)来去除重复的检测。<br>　　视觉社区多年来一直认为情景信息和对象间的关系有助于对象识别。大多数此类工作是在深度学习盛行之前做的。在深度学习时代，关于利用对象关系用于检测学习方面还有有取得显著的进步。大多数方法依旧将注意力集中在分别进行对象识别上。<br>　　一个原因是，难以将对象与对象之间的关系进行建模。对象在图像中的任意位置、具有不同的尺度、具有不同的类别，并且对象的数量也因图像而异。现代基于CNN的方法大多具有一种简单的固定网络结构。目前还不清楚如何解决现有方法中的上述不规范问题。<br>　　我们的方法受自然语言处理领域注意力模型成功的驱动。一个注意力模型可以通过从一组元素（例如：原句中的所有单词）中聚合信息（或特征）来影响一个独立的元素（例如：在神经机器翻译中，一个词在一个目标句子中）。聚合权重由目标任务驱动自动学习。注意模块可以模拟元素之间的依赖关系，而不会对其位置和特征分布进行过多的假设。目前，注意力模型已经成功应用于视觉问题，例如：图像字幕。<br>　　在本文中，我们第一次提出了一种适合对象检测的注意力模型。它建立在一个基础的注意力模型之上。一个明显的区别是，原始的元素是对象而不是单词。这些对象具有二维空间排列，以及尺度和宽高比的变化。从一般意义上来说，它们的位置或者是几何特征比一维句子中单词的位置起更复杂、更重要的作用。因此，所提出的模型将原来的注意力权值分为两个部分：原始权重和新的几何权重。后者建模了对象之间的空间关系，并且仅考虑他们之间的相对几何关系，使模块具有平移不变的理想对象识别属性。我们的实验证明了这种新的几何权重是重要的。<br>　　这个模型称为对象关系模型。它具有注意力模型相同的优点。它接受可变数量的输入，并行运行（与顺序关系模型相反），是完全可微分的，且保持原状（输入输出之间没有维度的变化）。因此，它可以作为一个基本的构建模块，灵活地适用于任何架构。</p>
<center><br><br><img src="http://blog.adminlee.com/18-6-19/24010109.jpg" alt=""><br>图1. 目前最先进的目标检测器是基于四步过程的。我们的对象关系模型（由红虚线框表示）可以方便的用于改进实例识别和删除重复步骤，从而形成端到端的对象检测器</center>

<p>　　特别的，它被应用到一些最先进的目标检测架构中，并且表现出一致的性能提升。如图1所示，它用于改进实例识别步骤并学习副本删除步骤（详见第4.1节）。对于实例识别。关系模块能够对所有对象进行联合推理并提高识别的准确性（4.2节）。对于副本删除，传统的非最大抑制（NMS）被轻量级的对象关系网络所取代和改进（4.3节），形成了第一个端到端的对象检测器（4.4节）。<br>　　原则上，我们的方法与大多数基于CNN的对象检测方法有着根本的不同，它利用了一个新的维度：一组对象被同时处理、推理且相互影响，而不是进行单独的识别。<br>　　这个对象关系模型是通用的，不局限与对象检测。我们没有任何理由阻止它在视觉任务中得到更广泛的应用，例如实例分割、动作识别、对象关系检测、标题、VQA等。源码可在从<a href="https://github.com/msracver/Relation-Networks-for-Object-Detection获得。" target="_blank" rel="external">https://github.com/msracver/Relation-Networks-for-Object-Detection获得。</a></p>
<h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h1><p>　　<strong>后处理过程中的对象关系</strong> 早期的研究使用对象关系作为后处理步骤，通过考虑对象关系对检测到的对象进行重新评分。例如：DPM使用co-occurrence指出两个对象可能存在于同一图像中的可能性，以获取对象分数。随后的方法考虑更多复杂的关系模型，考虑更多的位置和大小。推荐读者阅读文献[<a href="https://www.researchgate.net/publication/222680214_Context_based_object_categorization_A_critical_survey" target="_blank" rel="external">18</a>]以获取更详细的信息。这些模型在前深度学习时代取得了一定的成功，但是没有证明在深度卷积网络中也同样有效。一种可能的原因是深度卷积网络通过大的感受野隐式的包含了上下文信息。<br>　　<strong>顺序关系建模</strong> 最近的研究进行了顺序推理（LSTM和空间记忆网络（SMN））来建立关系网络。在检测时，早期被检测出的对象被用来帮助寻找下一个对象。使用这样的方法进行训练通常是复杂的。更重要的是，这个简单的前向网络没有给出改善最先进对象检测方法的证明。<br>　　相比之下，我们的方法对多个对象是并行的，且它很自然的适用并改善了现代对象检测器。<br>　　<strong>人类为中心的场景</strong> 有一些研究将精力集中在人类对象关系上，他们通常需要额外的关系注释，例如人类动作。相比之下，我们的方法适用对象-对象关系，并且不需要额外的监督。<br>　　<strong>去重</strong> 尽管适用深度学习进行对象检测有了重大的进展，但是对于这类任务而言，最有效的方法还是贪婪算法和手工设计非最大抑制以及他们的其他版本。这类任务自然需要关系建模，例如：非最大抑制在边间框和得分之间使用简单的关系。<br>　　目前，GossipNet试图通过将一组对象作为一个整体进行处理来学习去重，因此和我们的想法一致。然而，他们的网络是位专门的任务设计的，并且非常的复杂（深度&gt;80）.其精度可以和MNS相媲美，但是计算成本高。尽管原则上是允许进行端到端学习的，但是没有提供任何实验证明。<br>　　相比之下，我们的关系模型非常简单、通用且适用于去重。我们的网络对于去重更简单，且计算开销小，胜过SoftMNS。更重要的是，我们展示了端到端的对象检测学习是可行且有效的，这是第一次。<br>　　<strong>NLP中的注意力模型一届物理系统建模</strong> 注意力模型已将成功的应用在NLP领域和物理系统建模。在这些问题上，注意力模型可以很好的捕捉长期的依赖关系。在NLP中，最近有一种趋势是使用注意力模型来代替循环神经网络，以进行并行实现和更有效的学习。<br>　　我们的额方法受到了这些研究的启发。我们将注意力模型扩展到了对象检测的重要问题中。对于视觉对象关系建模，从一般意义上来说，它们的位置，或者几何特征起着复杂且重要的作用。因此，新提出的模型引用了一个新的几个权重来捕获对象间的空间关系。新的几何权重具有平移不变性，对于视觉模型，这是一个非常重要的性质。</p>
<blockquote>
<hr>
<p><strong>Algorithm 1</strong> Object relation module. Input is $N$ objects $\{(f{^n_A}，f{^n_G})\}{^N_{n=1}}$. Dimension of appearance feature $f_A$ is $d_f$. After each algorithm line is the computation complexity.</p>
<hr>
<p>1: <strong>hyper param</strong>: number of relations $N_r$<br>2: <strong>hyper param</strong> $d_k$: key feature dimension<br>3: <strong>hyper param</strong> $d_g$: geometric feature embedding dimension<br>4: <strong>learnt weights</strong>: $\{W{^r_K},W{^r_Q},W{^r_V},W{^r_G}\}{^{N{_r}}_{r=1}}$<br>5: <strong>for</strong> every $(n,r)$ <strong>do</strong> ………………………………….$O(N N_r)$<br>6:　　computer $\{\omega{^{mn,r}_G}\}{^N_{m=1}}$ using Eq.(5)……$O(N d_g)$<br>7:　　computer $\{\omega{^{mn,r}_A}\}{^N_{m=1}}$ using Eq.(4)……$O(d_k(2d_f+N))$<br>8:　　computer $\{\omega{^{mn,r}}\}{^N_{m=1}}$ using Eq.(3)……$O(N d_g)$<br>9:　　computer $f{^r_R}(n)$ using Eq.(2)………………$O(d{^2_f/N_r+N d_f/N_r})$<br>10: <strong>end for</strong><br>11: <strong>output</strong> new feature $\{f{^n_A}\}{^N_{n=1}}$ using Eq.(6)</p>
<hr>
</blockquote>
<h1 id="3-对象关系模型"><a href="#3-对象关系模型" class="headerlink" title="3 对象关系模型"></a>3 对象关系模型</h1><p>　　我们首先回顾一下被称为“Scaled Dot-ProductAttention”的基本注意力模型，输入由维度为$d_k$的查询和键及维度为$d_v$的值组成，在查询和所有键之间进行点击来获得它们的相似性，softmax函数用于获取值上的权重。给出一个查询$\text{q}$，所有的键（打包成矩阵$K$）和值（打包成$V$），输出值是输入值的加权平均。<br>$$\begin{equation} v^{out}=softmax(\frac{qK^t}{\sqrt{d_k}})V \end{equation}.$$<br>　　我们现在描述对象关系的计算。使一个对象由它的几何特征$\text{f}_G$和外观特征$\text{f}_A$组成。在这项工作中，$\text{f}_G$是一个简单的四维对象边界框，$\text{f}_A$由具体任务决定（4.2节和4.3节）。<br>　　输入一组$N$个对象$\{\text{f}{^n_A},\text{f}{^n_G}\}{^N_{n=1}}$，整组对象对于第$n$个对象的关系特征$\text{f}_R(n)$由下列公式计算：<br>$$\begin{equation} \text{f}_R(n)=\sum_m \omega^{mn} \cdot (W_V \cdot \text{f}{^m_A})\end{equation}.$$<br>　　输出是其他对象外观特征的加权和，由$W_V$（对应公式1中$V$的值）进行线性变换。关系权重$\omega^{mn}$表示其他对象的影响。由下面的公式计算得到。<br>$$\begin{equation} \omega^{mn}=\frac{\omega{^{mn}_G} \cdot \exp(\omega{^{mn}_A})}{\sum{_k}\omega {^{kn}_G} \cdot \exp(\omega{^{kn}_A})} \end{equation}.$$<br>　　外观权重$\omega {^{mn}_A}$是一个点积，与公式1相似，<br>$$\begin{equation} \omega{^{mn}_A} = \frac{dot(W_K\text{f}{^m_A},W_Q\text{f}{^n_A})}{\sqrt{d_k}} \end{equation}.$$<br>　　$W_K$和$W_Q$都是矩阵，与公式1中的$K$和$Q$起相同的作用。它们将原始特征$\text{f}{^m_A}$和$\text{f}{^n_A}$投影到子空间来衡量它们的匹配程度。投影后的特征维度为$d_k$。</p>
<center><br><br><img src="http://blog.adminlee.com/18-6-27/6106452.jpg" alt=""><br>图2. 左：公式6所述对象关系模型；右：公式2计算的关系特征。</center>

<p>　　几何特征有如下所示公式进行计算：<br>$$\begin{equation} \omega{^{mn}_G}=\max(0, W_G \cdot \mathcal{E}_G(\text{f}{^m_G},\text{f}{^n_G})) \end{equation}.$$<br>　　这里有两个步骤。第一，两个对象的几何特征被嵌入到高纬度的表示中，表示为$\mathcal{E}_G$。为了让它不受平移和尺度变换的影响，使用了一个4维的相对几何特征，即$((\log{\frac{\mid x_m-x_n \mid}{w_m}}), \log(\frac{\mid y_m-y_n \mid}{h_m}), \log(\frac{w_n}{w_m}), \log(\frac{h_n}{h_m}))^T$。这个4维特征使用文献[<a href="">56</a>]中的方法被嵌入到一个高维的表示，嵌入后的特征维度为$d_g$。<br>　　第二，嵌入的特征由$W_G$使用ReLU非线性转换为标量的权重并修剪为0。0修剪操作仅用于特定几何关系的对象之间的关系。<br>　　在注意力权重Eq.(3)中使用几何权重Eq.(5)使我们的方法与基本注意力Eq.(1)区分开来。为了验证公式5的有效性，我们在另外两种简单的变体上进行了实验。第一种变体称为none，它没有使用几何权重公式5，在公式3中，$\omega{^{mn}_G}$为常数1.0。第二种变体称为unary，它遵循目前的方法，特别的是，$\text{f}_G$被使用相同的方式嵌入到一个高维度（和$\text{f}_A$相同）空间，并被加到$\text{f}_A$上以形成新的外观特征。注意力权重随后采用none的方法进行计算。表1（a）和5.2节证明了几何权重的有效性。<br>　　对象关系模型聚合了所有的$N_r$个关系特征，并且通过加法增加输入对象的外观特征：<br>$$\begin{equation} \text{f}{^n_A} = \text{f}{^n_A} + \text{Concat}[\text{f}{^1_R}(n),…,\text{f}{^{N_r}_R}(n)],\text{for all }n \end{equation}.$$<br>　　Concat(·)用于聚合多个关系特征。为了匹配通道维度，每个$W{^r_V}$的输出通道设置为输入特征$\text{f}{^m_A}$的维度的$\frac{1}{N_r}$。<br>　　算法1中总结了对象关系模型公式6。它很容易使用基本操作实现，如图2所示。公式2中的每一个关系函数都被四个矩阵$(W_K,W_Q,W_G,W_V)$参数化，总得$4N_r$。让$d_f$为输入特征$\text{f}_A$的维度，则参数量为：<br>$$\begin{equation} O(\text{Space}) = N_r(2d_fd_k+d_g)+d{^2_f} \end{equation}.$$<br>　　根据算法1，其计算复杂度为<br>$$\begin{equation} O(\text{Comp.}) = Nd_f(2N_rd_k+d_f)+N^2N_r(d_g+d_k+d_f/N_r+1) \end{equation}.$$<br>　　典型参数值为$N_r=16,d_k=64,d_g=64$。通常，$N$和$d_f$在数百大小。当应用在现代对象检测器中，总的计算开销较低。<br>　　这个关系网络具有相同的输入输出维度，因此可视为在任何网络体系结构内使用的基本模块。由于它是完全可微的，因此可很容易的使用反向传播进行优化。接下来将其应用于现代对象检测系统中。</p>
<h1 id="4-关系网络用于对象检测"><a href="#4-关系网络用于对象检测" class="headerlink" title="4 关系网络用于对象检测"></a>4 关系网络用于对象检测</h1><h2 id="4-1-对象检测过程的回顾"><a href="#4-1-对象检测过程的回顾" class="headerlink" title="4.1 对象检测过程的回顾"></a>4.1 对象检测过程的回顾</h2><p>　　该研究符合基于区域的对象检测范式。这个范式建立在开创性的R-CNN上，并且包括目前主流的对象检测器。在之前所有的工作中都是用了4步处理过程。<br>　　第一步产生整个图像的特征。输入图像，通过一个深度卷积的骨干网络提取整分辨率的卷积特征（通常是输入图像分辨率的1/16）。骨干网络在ImageNet分类任务中预训练得到，并在检测训练时进行微调。<br>　　第二步生成区域特征。从卷积特征和一组稀疏的区域建议，经过一个RoI池化层为每一个区域建议提取固定分辨率（例如：7$\times$7）的区域特征。<br>　　第三步进行实例识别。从每一个建议的区域特征，由一个head网络预测建议属于某一对象类别的概率，并通过回归改善建议的边界框。这个网络通常是浅层的，随机初始化，并且在检测器训练时联合骨干网络一起训练。<br>　　最后一步进行去重。由于每一个对象只能被检测一次，因此应该删除对同一对象的重复检测。这通常作为启发式后处理步骤实施，称为非最大抑制（NMS）。在实际中，尽管NMS表象的很好，但是它是手工设计的、且欠佳。它组织了对象检测进行端到端学习。<br>　　在我们的研究中，所提出的对象关系网络用于最后两个步骤。我们证明了它增强了实例识别（4.2节）并学习到了去重（4.3节）。这两步不管是独立训练还是联合训练都很容易（4.4节）。联合训练进一步提升了准确率并且产生了第一个端到端对象检测系统。<br>　　<strong>不同结构的实现</strong> 为了验证我们方法的有效性和一般性，我们试验了不同先进骨干网络（ResNet）与性能最好的Faster R-CNN、FPN和DCN的组合。RPN网络用于生成建议。</p>
<ul>
<li>Faster R-CNN。它直接建立在像ResNet这样的骨干网络上，然后，RPN应用在conv4的卷积特征图上，接着，实例识别应用在conv5后的新的256维的1$\times$1的卷积层上。注意：conv5中的步长由2变成了1，这是常见的做法。</li>
<li>FPN。相比于Faster R-CNN，它通过增加一个自顶向下的侧向连接来构建促进不同尺度的端到端学习的特征金字塔修改了骨干网络。RPN和head网络在金字塔中应用部所有尺度的特征。我们根据文献[<a href="">37</a>]的训练细节进行试验。</li>
<li>DCN。相比于Faster R-CNN，它通过用可变形的卷积层替换conv5中的最后几个卷积层来修改骨干网络。它还将标准的RoI池化替换成可变形RoI池化。我们根据文献[<a href="">11</a>]的训练细节进行试验。</li>
</ul>
<p>　　尽管有一些差异，但是在上面的结构中有一个共性：它们都采用了相同的head网络结构，也就是说，RoI池化后的特征经过两个全连接层生成最终用于建议分类和边界框回归的特征。<br>　　接下来，我们证明关系模型能增强使用2fc head网络的实例识别步骤。</p>
<h2 id="4-2-关系用于实例识别"><a href="#4-2-关系用于实例识别" class="headerlink" title="4.2 关系用于实例识别"></a>4.2 关系用于实例识别</h2><p>　　给出第n个建议RoI池化后的特征，应用两个1024维的全连接层。实例分类和边界框回归通过一个线性层实现。处理过程如下：<br>$$\begin{equation} \begin{aligned} RoI_Feat_n &amp; \overset{FC}{\longrightarrow} &amp; 1024 \ &amp;\overset{FC}{\longrightarrow} &amp; 1024 \ <br>&amp; \stackrel{\underrightarrow{LINEAR}}{} &amp;(score_n,bbox_n) \end{aligned} \end{equation}$$<br>　　对象关系网络（第3章）可以转换所有建议1024维的特征而不改变特征维度。因此，它可以在公式（9）中的任何一个fc层之后使用任意次数。这种增强的$2fc+RM$（RM表示关系模型）head如图3所示，其过程如下：<br>$$\begin{equation} \begin{aligned} RoI_Feat{^N_{n=1}} &amp; \overset{FC}{\longrightarrow} &amp; 1024 \cdot \mid N  \stackrel{\underrightarrow{\{RM\}{^{r_1}}}}{} 1024 \cdot N \ &amp;\overset{FC}{\longrightarrow} &amp; 1024 \cdot N  \stackrel{\underrightarrow{\{RM\}{^{r_2}}}}{} 1024 \cdot N \ <br>&amp; \stackrel{\underrightarrow{LINEAR}}{} &amp;\{(score_n,bbox_n)\}{^N_{n=1}} \end{aligned} \end{equation}$$<br>　　在公式（10）中$r_1$和$r_2$表示关系模型重复多少次。注意：关系模型需要所有建议的边界框作为输入，这个记号在此被忽略了。<br>　　增加关系模型可以有效地提高实例识别的准确率。这是通过实验中（5.1节）的综合消去研究证明的。</p>
<h2 id="4-3-关系网络和去重"><a href="#4-3-关系网络和去重" class="headerlink" title="4.3 关系网络和去重"></a>4.3 关系网络和去重</h2><p>　　去重任务自然需要利用对象之间的关系。启发式的NMS方法是个简单的例子：具有最高得分的对象将移除附近（几何关系）具有低得分（分数关系）的对象。<br>　　尽管这种方法很简单，但是NMS中贪婪的性质和手工选择的参数使得该方法称为了次优选择。下面，我们提出的对象关系模型以一种更简单有效的方式学习去重。<br>　　去重是一个二分类问题。对于每一个真实对象，只有一个检测到的对象与之匹配才被分类成正确的，其他的匹配被分类为重复。</p>
<center><br><br><img src="http://blog.adminlee.com/18-7-19/76449911.jpg" alt=""><br>图3. 增强的2fc head （a），采用对象关系网络的去重分类网络（b）<br></center>

<p>　　这个分类通过图3（b）中的网络实现。输入是一组被检测到的对象（实例识别公式（9）或公式（10）的输出），每一个对象都具有1024维的最终特征、分类得分$s_0$和边界框。网络输出是每一个对象的二元分类概率$s_1 \in [0,1]$(1表示正确，0表示重复)，两个得分的积$s_0s_1$为最终的分类得分。因此，一个较好的检测应该具有两个较高的得分。<br>　　这个网络具有3个步骤。首先将1024维的特征与分类得分融合生成表现特征；然后关系模型转换所有对象的这种表现特征；最后，每个对象转换后的特征经过一个线性分类器（图3（b）中$W_s$）和Sigmoid输出概率$\in [0,1]$。<br>　　关系模型是网络的核心。它可以使用多个源的信息进行有效的端到端学习，此外，分类分数的使用也很重要。<br>　　<strong>排序特征</strong> 我们发现将得分转换为排序比使用得分的值更有效，具体地说，输入Ｎ个对象按照其分数的降序排列。每个对象相应的都给出一个排序$\in [1,N]$。然后使用与第3章中几何特征嵌入相同的方法将排序标量嵌入到具有128维的高维特征中。<br>　　排序特征和原始的1024维表现特征都被转换为128维（分别通过图3（b）中的$W_{fR}$和$W_f$完成），并作为关系网络的输入。<br>　　<strong>哪个对象正确？</strong> 给出一系列检测到的对象，不能很清楚的知道哪个对象与真实对象匹配是正确的。最明显的选择就是遵循PASCAL VOC或COCO数据集的评价标准。即：针对检测到的边界框和真实边界框的IoU给出一个预定义的阈值$\eta$，首先所有检测到的边界框中$\text{IoU} \geq \eta$的认为与真实对象匹配一致，然后匹配一致的边界框中，得分最高的即为正确，其余均为重复。<br>　　因此，当学习和评估使用相同的$\eta$时，这种选择标准效果最好。例如：在学习过程中使用$\eta =0.5$，如表4中<a href="mailto:mAP@0.5" target="_blank" rel="external">mAP@0.5</a>所示。<br>　　这个结果揭示了我们的方法中一个独一无二的优点，即：不使用MNS，去重步骤可以根据需要自适应的学习，取代使用预设参数的方法。例如：当需要获得高的定位准确率时，需要预设一个大的$\eta$值。<br>　　受COCO的评估标准启发，我们最好的实践同时使用多个阈值，即：$\eta \in \{0.5,0.6,0.7,0.8,0.9\}$。明显地，在图3（b）中，对分类器$W_s$进行了更改，以输出与不同IoU阈值和正确检测对应的多个概率，从而产生多个二元分类损失项。训练可以很好的平衡不同情况，在推理阶段，多个概率简单平均作为一个单独的输出。<br>　　<strong>训练</strong> 二元交叉熵损失用于最后得分（两个得分的积，如图3（b）所示）。损失是所有类别上所有检测边界框的平均损失。为所有对象训练一个网络。<br>　　注意：重复分类问题是极其不平衡的。大多数的检测都是重复的，正确检测的比率通常小于0.01。尽管如此，我们发现简单的交叉熵损失效果良好。这归因于最后得分$s_0s_1$的乘法操作。由于大多是检测具有较小的$s_0$（大部分小于0.01），因此具有较小的$s_0s_1$。它们的损失值$L=-\log (1-s_0s_1)$和反向传播梯度$\partial L / \partial s_1 =s_0 / (1-s_0s_1)$的量级都非常小并且对优化影响不大。直观地，训练关注小部分具有大$s_0$的真实重复检测。这与最近的焦点损失研究具有相似的特点，即大多是无关紧要的损失项都向下加权，在优化过程中起次要作用。<br>　　<strong>推理</strong> 对所有对象单独使用相同的去重网络。乍一看，当对象类别数量（COCO数据集有80类）和建议数量（$N=300$）都较大时时，系统运行的复杂性可能会较高。然而，实际上大多数区域建议的最初的分$s_0$在大多数对象类别上都接近0。例如：在如表4所示的实验中，仅有12.0%的类具有区域建议得等大于0.01，并且在这些类别中，仅有6.8%的区域建议得分大于0.01。<br>　　在去除这些无关紧要的类别和区域建议后，最终的识别精度不受影响。在剩下的区域建议中运行去重网络是可行的，在单块Titan X GPU上约2ms。而NMS和SoftNMS方法是连续的且在单块GPU上的运行时间是5ms。并且，目前学习型的NMS使用非常深且复杂的网络（深度大于80），这种方法的效率大大低于我们的方法。</p>
<h2 id="4-4-端到端对象检测"><a href="#4-4-端到端对象检测" class="headerlink" title="4.4 端到端对象检测"></a>4.4 端到端对象检测</h2><p>　　在4.3节中的去重网络是独立训练的，然而，没什么能阻止进行端到端的训练。如图3（b）中的红色箭头所示，反向传播的梯度可以通过原始的1024维特征和分类得分，这可以进一步反向传播到head和骨干网络。<br>　　我们的端到端训练简单的联合区域建议损失、4.2节中的实例识别损失和4.3节中的重复分类损失，且3者具有相同的权重。对于实例识别，使用了原始的head公式（9）和增强的head公式（10）。<br>　　端到端训练理论上是可行的，但是是否有效呢？乍一看，具有两个问题。<br>　　第一，实例识别步骤和去重步骤的目标看起来似乎是矛盾的。前者希望所有的对象匹配到相同的真实对象以获得高得分，而后者希望仅有一个与之匹配。在我们的实验中，相比于4.2节和4.3节中的单独训练，我们发现端到端训练效果良好，并且两个网络的收敛速度都很快。我们相信这种看似矛盾的冲突通过最终得分$s_0s_1$的乘法操作得到了调和，使得这两个网络的目标互补而不是冲突。在实例识别步骤中，仅仅需要处理具有高得分$s_0$的好的区域建议（不管是不是重复）。在去重步骤则只需处理具有低得分$s_1$的重复区域建议。只要两个分数中的一个是正确的，大多数非对象或重复区域建议是正确的。<br>　　第二，去重步骤中的真实标签依赖于实例识别步骤中的输出，并在端到端训练的过程中改变。然而，在实验中我们没有观察到任何有这种不稳定性产生的副作用。虽然目前还没有理论依据，但我们的猜测是重复删除网络相对容易训练，不稳定标签可能使作为一种正规化的手段。<br>　　通过实验验证（5.3节），端到端训练提升了识别准确率。</p>
<h1 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h1><p>　　所有实验都是在具有80类对象类别的COCO数据集上进行的。训练使用具有80K图像训练图像和35K验证图像的合集。大多数的消除实验的检测准确度是在未使用的5K验证图像上产生的（用minival表示），这是一种常用做法。为了进行系统级的比较，表5给出了在test-dev数据集上的准确度。<br>　　对于骨干网络，我们使用了ResNet-50和ResNet-101，除非特别说明，一律使用ResNet-50。<br>　　对于Faster R-CNN和DCN，我们的训练主要参考文献[<a href="">11</a>]，对于FPN，我们的训练主要参考文献[<a href="">37</a>]。详见附录。</p>
<h2 id="5-1-关系用于实例识别（Relation-for-Instance-Recognition）"><a href="#5-1-关系用于实例识别（Relation-for-Instance-Recognition）" class="headerlink" title="5.1 关系用于实例识别（Relation for Instance Recognition）"></a>5.1 关系用于实例识别（Relation for Instance Recognition）</h2><p>　　在本节，NMS使用0.6的IoU阈值对所有实验进行去重。<br>　　<strong>关系模型改善实例识别</strong> 表1在不同参数下比较了公式（9）中的基线$2fc  head$网络和公式（10）中的$2fc+RM  head$网络。<br>　　首先，和文献比较，我们的基线模型获得了一个合理的准确率（29.6mAP）（例如：文献[<a href="">37</a>]使用ResNet-50获得了28.0的mAP，文献[<a href="">11</a>]使用ResNet-101获得了29.4的mAP）。<br>　　消除研究主要表现在三个关键参数。<br>　　<em>几何特征的使用</em>。如第3章中的分析，我们在公式（5）中使用的几何特征与两种普通实现进行了比较。结果表明我们所提出的方法效果最好，虽然三者的结果都超越了基线模型的结果。<br>　　<em>关系$N_r$的数量</em>。使用更多的关系稳定的提升了准确率。在$N_r=16$时，提升达到了饱和，获得了2.3mAP的提升。<br>　　<em>模型数量</em>。使用更多的关系模型稳定的提升了准确率，达到了3.2mAP的提升。与默认的$r_1=1, r_2=1$相比，这同时增加了参数以及计算的复杂性。<br>　　<strong>准确率的提升是不是来自于更多的参数和更深的网络？</strong> 表2回答了该问题，通过在宽度和深度上增强$2fc  head（a）$基线模型，使其复杂性与添加关系模块的复杂性相当。<br>　　更宽的$2fc  head$（1432维（b））仅仅带来了很小的改善（+0.1mAP）。更深的$3fc  head(c)$降低了准确率（-0.6mAP），这可能是由于训练困难造成的。为了使训练变得简单，使用了残差块，但是也只获得了很小的改善（+0.3mAP），当使用全局对象关系时（即在分类前，2048维全局平均池化特征与第二个1024维的实例特征连接），没有一点改善。相比之下，我们的方法显著提升了准确度（+2.3mAP）。<br>　　我们也考虑了另一种基线模型，该模型将原始的池化特征与来自$2\times$大的RoI特征连接，性能从29.6提升到30.4，这表明该模型能有效的利用上下文关系信息。另外，我们将这种新的head网络与关系模型相结合，即：使用$\{r_1,r_2\}=\{1,1\}$（h）替换$2fc$。获得了32.5mAP，这比（f）（31.9mAP）好0.6mAP。这表明使用大窗口上下文与关系网络是互补的。<br>　　当使用更多的残差块时，head网络变得更深（i），准确率不再提升。当使用更多的关系模型（j）时，准确率不断提升。<br>　　这个比较说明，关系模型时有效的，其效果超过了增加网络容量。<br>　　<strong>复杂性</strong> 在每个关系模型中，当$N_r=16$时，$d_f=1024,d_k=64,d_g=64$，一个模型具有3百万参数和12亿FLOPs，从公式7）和（8）开始。计算开销相对较小，整个网络的复杂性对比如表5所示（比Faster R-CNN和DCN小2%，比FPN小8%）。</p>
<h2 id="5-2-关系用于去重"><a href="#5-2-关系用于去重" class="headerlink" title="5.2 关系用于去重"></a>5.2 关系用于去重</h2><p>　　本节中的所有实验都使用表1中Faster R-CNN基线2fc head网络的区域建议来进行4.3节中的方法的训练和推理。<br>　　在我们的方法中，对象关系模型的参数设置为$d_f=128,d_k=64,d_g=64,N_r=16,N=100$。使用更大的值，准确率不再提升。去重网络具有33万的参数，大约3亿的FLOPs，计算开销很小，相比于使用ResNet-50的Faster R-CNN基线网络，该去重网络的模型大小和计算开销都在1%左右。<br>　　表3研究了不同输入特征对关系模型的影响（图3（b））。使用$\eta = 0.5$，我们的方法将mAP提升至30.3。当不使用排序特征时，mAP掉至26.6,；当使用相似的方法（得分被嵌入到128维特征中）用类别得分$s_0$替换排序特征时，mAP掉至28.3；当不使用1024维的表现特征时，mAP轻微的下降到29.9。这些结果表明，对于最终的准确率，排序特征是非常重要的。<br>　　当不使用几何特征时，mAP下降到28.1；当使用第三章和表1（a）所示的$unary$方法时，mAP下降到28.2。这些结果证明了我们使用的几何权重公式（5）的有效性。<br>　　<strong>和NMS相比</strong> 表4将我们的方法与NMS方法及其最好的变体SoftNMS方法进行了比较。这些都是用于去重的最先进的方法。<br>　　注意，这三种方法都有一个类似的参数来控制定位的精度：NMS中的IoU阈值$N_r$，SoftNMS中的正则化参数$\sigma$，以及我们方法中的真实标签标准参数$\eta$。在不同的定位指标下，改变这些参数就改变了准确率。然而，对于NMS方法，目前还不清楚应该怎样设置最优参数，而不是试错。我们的方法则很容易解释，因为参数$\eta$直接规定了定位准确率。$\eta=0.5$时记为$\text{mAP}_{50}$，$\eta=0.75$时记为$\text{mAP}_{75}$，$\eta \in [0.5,0.9]$时记为$\text{mAP}$。<br>　　最终的mAP准确率比NMS和SoftNMS都好，成为新的最好结果。在接下来的端到端实验中，使用$\eta \in [0.5,0.9]$。</p>
<h2 id="5-3-端到端对象检测"><a href="#5-3-端到端对象检测" class="headerlink" title="5.3 端到端对象检测"></a>5.3 端到端对象检测</h2><p>　　表4的最后一行比较了端到端学习和单独训练实例识别与去重。端到端学习提升了0.5mAP的准确率。<br>　　最后，我们在一些更强的骨干网络（例如：ResNet-101）和更好的检测架构（例如：表5中的FPN和DCN）上进行研究。使用采用ResNet-101骨干网络的Faster R-CNN，通过使用$2fc+RM  head$网络替换$2fc  head$网络，我们的方法在COCO minival数据集上提升了2.5mAP。进一步使用端到端训练去重网络，准确率进一步提升了0.5mAP。这种改善在COCO test-Dev数据集上的表现相同。在更强的基线模型上，例如：DCN和FPN，通过端到端训练特征增强网络和去重网络，在精度上也有适度的改善。注意，我们基线网络的实现与原始网络相比具有更高的准确率。（38.1 VS. 33.1，37.2 VS. 36.2）。</p>
<h1 id="6-总结"><a href="#6-总结" class="headerlink" title="6 总结"></a>6 总结</h1><p>　　综合消除实验表明，关系模型学习到了单个对象学习时丢失的对象间的信息。然而，不清楚在关系模型中学到了什么，尤其是当多个关系模型叠加时。<br>　　为了便于理解，我们研究了关系模型在$\{r_1,r_2\}=\{1,0\}$head 网络上的性能，如表1（c）所示。图4展示了一些具有高关系权重的典型例子。左边的例子表明，一些与同一个真实对象重叠的对象对定位对象中心有帮助。右边的例子表明人对手套做出了贡献。虽然这些例子都是直观的，但我们对关系模型是如何工作的理解是初步的，并将其作为未来的研究内容。</p>
<center><br><br><img src="http://blog.adminlee.com/18-7-22/26962631.jpg" alt=""><br>表1<br><img src="http://blog.adminlee.com/18-7-22/11324410.jpg" alt=""><br>表2<br><img src="http://blog.adminlee.com/18-7-22/12870677.jpg" alt=""><br>表3<br><img src="http://blog.adminlee.com/18-7-22/50028070.jpg" alt=""><br>表4<br><img src="http://blog.adminlee.com/18-7-22/33110178.jpg" alt=""><br>图4<br></center>
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>本文作者：</strong>
      ailee
    </li>
    <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://ailee.me/2018/06/18/Relation Networks for Object Detection/" title="Relation Networks for Object Detection">http://ailee.me/2018/06/18/Relation Networks for Object Detection/</a>
    </li>
    <li class="post-copyright-license">
      <strong>版权声明： </strong>
      本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
    </li>
  </ul>


      
    </div>

	<div>
	  
		<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
	  
	</div>
	
    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/目标检测/" rel="tag"># 目标检测</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/关系网络/" rel="tag"># 关系网络</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/15/Caffe-Python接口常用API参考/" rel="next" title="Caffe-Python接口常用API总结">
                <i class="fa fa-chevron-left"></i> Caffe-Python接口常用API总结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/09/10/Python文件及文件夹操作/" rel="prev" title="利用Python对文件及文件夹进行相关操作">
                利用Python对文件及文件夹进行相关操作 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!--MOB SHARE BEGIN-->
<div class="-mob-share-ui-button -mob-share-open">分享</div>
<div class="-mob-share-ui" style="display: none">
    <ul class="-mob-share-list">
        <li class="-mob-share-weibo"><p>新浪微博</p></li>
        <li class="-mob-share-weixin"><p>微信</p></li>
        <li class="-mob-share-qzone"><p>QQ空间</p></li>
        <li class="-mob-share-qq"><p>QQ好友</p></li>
        <li class="-mob-share-douban"><p>豆瓣</p></li>
    </ul>
    <div class="-mob-share-close">取消</div>
</div>
<div class="-mob-share-ui-bg"></div>
<script id="-mob-share" src="http://f1.webshare.mob.com/code/mob-share.js?appkey=236fe84842da6"></script>
<!--MOB SHARE END-->
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8yOTc5Ny82MzYz"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="ailee" />
          <p class="site-author-name" itemprop="name">ailee</p>
           
              <p class="site-description motion-element" itemprop="description">优秀不够，一定要卓越，一定要无可替代才是最重要的。</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">31</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">18</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">49</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              友情链接
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="https://rootingc.me" title="rooting" target="_blank">rooting</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://mindthink.me/" title="mindthink" target="_blank">mindthink</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.yoogu.cc/" title="Wakke Wang" target="_blank">Wakke Wang</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ailee</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      本站访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      本站总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  






  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("3JEOnLBbeCkTnDLXdRB4RQNh-gzGzoHsz", "LpdXae1Pn9iCAuW8OVklkOqp");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
