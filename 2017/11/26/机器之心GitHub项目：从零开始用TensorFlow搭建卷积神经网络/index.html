<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="深度学习,tensorflow," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="本文的重点是实现，并不会从理论和概念上详细解释深度神经网络、卷积神经网络、最优化方法等基本内容。但是机器之心发过许多详细解释的入门文章或教程，因此，我们希望读者能先了解以下基本概念和理论。当然，本文注重实现，即使对深度学习的基本算法理解不那么深同样还是能实现本文所述的内容。">
<meta name="keywords" content="深度学习,tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络【转】">
<meta property="og:url" content="http://yoursite.com/2017/11/26/机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络/index.html">
<meta property="og:site_name" content="心雨达致">
<meta property="og:description" content="本文的重点是实现，并不会从理论和概念上详细解释深度神经网络、卷积神经网络、最优化方法等基本内容。但是机器之心发过许多详细解释的入门文章或教程，因此，我们希望读者能先了解以下基本概念和理论。当然，本文注重实现，即使对深度学习的基本算法理解不那么深同样还是能实现本文所述的内容。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://otue1rxl3.bkt.clouddn.com/17-11-27/98620237.jpg">
<meta property="og:image" content="http://otue1rxl3.bkt.clouddn.com/17-11-27/10952232.jpg">
<meta property="og:image" content="http://otue1rxl3.bkt.clouddn.com/17-11-27/91538923.jpg">
<meta property="og:image" content="http://otue1rxl3.bkt.clouddn.com/17-11-28/51381705.jpg">
<meta property="og:image" content="http://otue1rxl3.bkt.clouddn.com/17-11-28/39600002.jpg">
<meta property="og:image" content="http://otue1rxl3.bkt.clouddn.com/17-11-28/88148369.jpg">
<meta property="og:image" content="http://otue1rxl3.bkt.clouddn.com/17-11-28/13366246.jpg">
<meta property="og:updated_time" content="2017-11-28T14:24:44.606Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络【转】">
<meta name="twitter:description" content="本文的重点是实现，并不会从理论和概念上详细解释深度神经网络、卷积神经网络、最优化方法等基本内容。但是机器之心发过许多详细解释的入门文章或教程，因此，我们希望读者能先了解以下基本概念和理论。当然，本文注重实现，即使对深度学习的基本算法理解不那么深同样还是能实现本文所述的内容。">
<meta name="twitter:image" content="http://otue1rxl3.bkt.clouddn.com/17-11-27/98620237.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/11/26/机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络/"/>





  <title>机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络【转】 | 心雨达致</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?1e9e36dfff2baf430f723f50d54d376e";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">心雨达致</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/26/机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ailee">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="心雨达致">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络【转】</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-26T21:55:04+08:00">
                2017-11-26
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2017-11-28T22:24:44+08:00">
                2017-11-28
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2017/11/26/机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络/" class="leancloud_visitors" data-flag-title="机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络【转】">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          
              <div class="post-description">
                  本文的重点是实现，并不会从理论和概念上详细解释深度神经网络、卷积神经网络、最优化方法等基本内容。但是机器之心发过许多详细解释的入门文章或教程，因此，我们希望读者能先了解以下基本概念和理论。当然，本文注重实现，即使对深度学习的基本算法理解不那么深同样还是能实现本文所述的内容。
              </div>
          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="1-1-张量和图"><a href="#1-1-张量和图" class="headerlink" title="1.1 张量和图"></a>1.1 张量和图</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 引入 tensorflow</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义两个变量</span></div><div class="line">a = tf.constant(<span class="number">2</span>, tf.int16)</div><div class="line">b = tf.constant(<span class="number">4</span>, tf.float32)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义一张图</span></div><div class="line">graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> graph.as_default():</div><div class="line">    <span class="comment"># 定义两个变量</span></div><div class="line">    a = tf.Variable(<span class="number">8</span>, tf.float32)</div><div class="line">    b = tf.Variable(tf.zeros([<span class="number">2</span>,<span class="number">2</span>]), tf.float32)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># </span></div><div class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</div><div class="line">    tf.global_variables_initializer().run()</div><div class="line">    <span class="comment"># print(f)</span></div><div class="line">    print(session.run(a))</div><div class="line">    print(session.run(b))</div></pre></td></tr></table></figure>
<pre><code>8
[[ 0.  0.]
 [ 0.  0.]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 声明一个 2 行 3 列的变量矩阵，该变量的值服从标准差为 1 的正态分布，并随机生成</span></div><div class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 应用变量来定义神经网络中的权重矩阵和偏置项向量：</span></div><div class="line"><span class="comment"># truncated_normal()</span></div><div class="line"><span class="comment"># Outputs random values from a truncated normal distribution（截断正态分布）.</span></div><div class="line">weights = tf.Variable(tf.truncated_normal([<span class="number">256</span> * <span class="number">256</span>, <span class="number">10</span>]))</div><div class="line">biases = tf.Variable(tf.zeros([<span class="number">10</span>]))</div><div class="line">print(weights.get_shape().as_list())</div><div class="line">print(biases.get_shape().as_list())</div></pre></td></tr></table></figure>
<pre><code>[65536, 10]
[10]
</code></pre><h1 id="1-2-占位符和feed-dict"><a href="#1-2-占位符和feed-dict" class="headerlink" title="1.2 占位符和feed_dict"></a>1.2 占位符和feed_dict</h1><p>占位符并没有初始值，它只会分配必要的内存。在会话中，占位符可以使用 feed_dict 馈送数据。</p>
<p>feed_dict 是一个字典，在字典中需要给出每一个用到的占位符的取值。在训练神经网络时需要每次提供一个批量的训练样本，如果每次迭代选取的数据要通过常量表示，那么 TensorFlow 的计算图会非常大。因为每增加一个常量，TensorFlow 都会在计算图中增加一个结点。所以说拥有几百万次迭代的神经网络会拥有极其庞大的计算图，而占位符却可以解决这一点，它只会拥有占位符这一个结点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">w1 = tf.Variable(tf.random_normal([<span class="number">1</span>, <span class="number">2</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</div><div class="line"><span class="comment"># 因为需要重复输入x，而每建一个x就会生成一个结点，计算图的效率会低。所以使用占位符</span></div><div class="line">x = tf.placeholder(tf.float32, shape=(<span class="number">1</span>,<span class="number">2</span>))</div><div class="line">x1 = tf.constant([<span class="number">0.7</span>, <span class="number">0.9</span>])</div><div class="line"></div><div class="line">a = x + w1</div><div class="line">b = x1 + w1</div><div class="line"></div><div class="line">sees = tf.Session()</div><div class="line">sees.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line"><span class="comment"># 运行y时将占位符填上，feed_dict为字典，变量名不可变</span></div><div class="line">y_1 = sees.run(a, feed_dict=&#123;x:[[<span class="number">0.7</span>, <span class="number">0.9</span>]]&#125;)</div><div class="line">y_2 = sees.run(b)</div><div class="line">print(<span class="string">"使用占位符计算"</span>)</div><div class="line">print(y_1)</div><div class="line">print(<span class="string">"常亮计算"</span>)</div><div class="line">print(y_2)</div><div class="line">sees.close</div></pre></td></tr></table></figure>
<pre><code>使用占位符计算
[[-0.11131823  2.38459873]]
常亮计算
[[-0.11131823  2.38459873]]





&lt;bound method BaseSession.close of &lt;tensorflow.python.client.session.Session object at 0x000000000CB44780&gt;&gt;
</code></pre><h2 id="下面是使用占位符的案例："><a href="#下面是使用占位符的案例：" class="headerlink" title="下面是使用占位符的案例："></a>下面是使用占位符的案例：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算两点间的欧氏距离</span></div><div class="line">list_of_points1_ = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>]]</div><div class="line">list_of_points2_ = [[<span class="number">15</span>,<span class="number">16</span>], [<span class="number">13</span>, <span class="number">14</span>], [<span class="number">11</span>,<span class="number">12</span>], [<span class="number">9</span>, <span class="number">10</span>]]</div><div class="line"></div><div class="line">list_of_points1 = np.array([np.array(elem).reshape(<span class="number">1</span>,<span class="number">2</span>) <span class="keyword">for</span> elem <span class="keyword">in</span> list_of_points1_])</div><div class="line">list_of_points2 = np.array([np.array(elem).reshape(<span class="number">1</span>,<span class="number">2</span>) <span class="keyword">for</span> elem <span class="keyword">in</span> list_of_points2_])</div><div class="line"></div><div class="line"><span class="comment"># 新建一张图</span></div><div class="line">graph = tf.Graph()</div><div class="line"></div><div class="line"><span class="keyword">with</span> graph.as_default():</div><div class="line">    <span class="comment"># 我们使用 tf.placeholder() 创建占位符 ，在 session.run() 过程中再投递数据 </span></div><div class="line">    point1 = tf.placeholder(tf.float32, shape=(<span class="number">1</span>,<span class="number">2</span>))</div><div class="line">    point2 = tf.placeholder(tf.float32, shape=(<span class="number">1</span>,<span class="number">2</span>))</div><div class="line">    </div><div class="line">    <span class="comment"># 定义一个函数</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate_eucledian_distance</span><span class="params">(point1, point2)</span>:</span></div><div class="line">        difference = tf.subtract(point1, point2)</div><div class="line">        power2 = tf.pow(difference, tf.constant(<span class="number">2.0</span>, shape=(<span class="number">1</span>,<span class="number">2</span>)))</div><div class="line">        <span class="comment"># 计算一个张量维数的总和</span></div><div class="line">        add = tf.reduce_sum(power2)</div><div class="line">        eucledian_distance = tf.sqrt(add)</div><div class="line">        <span class="keyword">return</span> eucledian_distance</div><div class="line">    </div><div class="line">    dist = calculate_eucledian_distance(point1, point2)</div><div class="line">    </div><div class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</div><div class="line">    tf.global_variables_initializer().run()</div><div class="line">    <span class="keyword">for</span> ii <span class="keyword">in</span> range(len(list_of_points1)):</div><div class="line">        point1_ = list_of_points1[ii]</div><div class="line">        point2_ = list_of_points2[ii]</div><div class="line">        </div><div class="line">        <span class="comment"># 使用feed_dict将数据投入到[dist]中</span></div><div class="line">        feed_dict = &#123;point1: point1_, point2: point2_&#125;</div><div class="line">        distance = session.run([dist], feed_dict=feed_dict)</div><div class="line">        print(<span class="string">"the distance between &#123;&#125; and &#123;&#125; -&gt; &#123;&#125;"</span>.format(point1_, point2_, distance))</div></pre></td></tr></table></figure>
<pre><code>the distance between [[1 2]] and [[15 16]] -&gt; [19.79899]
the distance between [[3 4]] and [[13 14]] -&gt; [14.142136]
the distance between [[5 6]] and [[11 12]] -&gt; [8.485281]
the distance between [[7 8]] and [[ 9 10]] -&gt; [2.8284271]
</code></pre><h2 id="解析一段构建了三层全连接神经网络的代码"><a href="#解析一段构建了三层全连接神经网络的代码" class="headerlink" title="解析一段构建了三层全连接神经网络的代码"></a>解析一段构建了三层全连接神经网络的代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> RandomState</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 每次迭代读取的批量为 10</span></div><div class="line">batch_size = <span class="number">10</span></div><div class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</div><div class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># None 可以根据batch 大小确定纬度，在shape 的一个纬度上使用None</span></div><div class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>))</div><div class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">1</span>))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 激活函数使用ReLU</span></div><div class="line"><span class="comment"># tf.nn.relu() 代表调用 ReLU 激活函数</span></div><div class="line"><span class="comment"># tf.matmul() 为矩阵乘法</span></div><div class="line">a = tf.nn.relu(tf.matmul(x,w1))</div><div class="line">yhat = tf.nn.relu(tf.matmul(a,w2))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义交叉熵为损失函数，训练过程使用Adam算法最小化交叉熵</span></div><div class="line"><span class="comment"># tf.clip_by_value(yhat,1e-10,1.0) 这一语句代表的是截断 yhat 的值</span></div><div class="line">cross_entropy = tf.reduce_mean(y * tf.log(tf.clip_by_value(yhat, <span class="number">1e-10</span>, <span class="number">1.0</span>)))</div><div class="line"><span class="comment"># tf.train.AdamOptimizer(learning_rate).minimize(cost_function) 是进行训练的函数，其中我们采用的是 Adam 优化算法更新权重，并且需要提供学习速率和损失函数这两个参数。</span></div><div class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(cross_entropy)</div><div class="line"></div><div class="line">rdm = RandomState(<span class="number">1</span>)</div><div class="line">data_size = <span class="number">512</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 生成两个特征，共data_size个样本</span></div><div class="line">X = rdm.rand(data_size, <span class="number">2</span>)</div><div class="line"><span class="comment"># 定义规则给出样本标签，所用x1+x2&lt;1的样本为正样本，其他为负样本，Y,1为正样本</span></div><div class="line">Y = [[int(x1+x2 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x1, x2) <span class="keyword">in</span> X]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 创建一个会话</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    print(sess.run(w1))</div><div class="line">    print(sess.run(w2))</div><div class="line">    steps = <span class="number">11000</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</div><div class="line">        <span class="comment"># 选取每一个批量读取的首位位置，确保在1个epoch内采样训练</span></div><div class="line">        start = i * batch_size % data_size</div><div class="line">        end = min(start + batch_size, data_size)</div><div class="line">        sess.run(train_step, feed_dict=&#123;x:X[start:end], y:Y[start:end]&#125;)</div><div class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</div><div class="line">            training_loss = sess.run(cross_entropy, feed_dict=&#123;x:X, y:Y&#125;)</div><div class="line">            print(<span class="string">"在迭代 %d 次后，训练损失为 %g"</span>%(i, training_loss))</div></pre></td></tr></table></figure>
<pre><code>[[-0.81131822  1.48459876  0.06532937]
 [-2.4427042   0.0992484   0.59122431]]
[[-0.81131822]
 [ 1.48459876]
 [ 0.06532937]]
在迭代 0 次后，训练损失为 -0.311019
在迭代 1000 次后，训练损失为 -11.1082
在迭代 2000 次后，训练损失为 -11.1082
在迭代 3000 次后，训练损失为 -11.1082
在迭代 4000 次后，训练损失为 -11.1082
在迭代 5000 次后，训练损失为 -11.1082
在迭代 6000 次后，训练损失为 -11.1082
在迭代 7000 次后，训练损失为 -11.1082
在迭代 8000 次后，训练损失为 -11.1082
在迭代 9000 次后，训练损失为 -11.1082
在迭代 10000 次后，训练损失为 -11.1082
</code></pre><p>上面的代码定义了一个简单的三层全连接网络（输入层、隐藏层和输出层分别为 2、3 和 1 个神经元），隐藏层和输出层的激活函数使用的是 ReLU 函数。该模型训练的样本总数为 512，每次迭代读取的批量为 10。这个简单的全连接网络以交叉熵为损失函数，并使用 Adam 优化算法进行权重更新。</p>
<h1 id="tensorflow中的神经网络"><a href="#tensorflow中的神经网络" class="headerlink" title="tensorflow中的神经网络"></a>tensorflow中的神经网络</h1><h2 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h2><p><img src="http://otue1rxl3.bkt.clouddn.com/17-11-27/98620237.jpg" alt=""></p>
<ul>
<li>1 定义一些函数，便于对数据进行预处理</li>
</ul>
<h2 id="2-2-导入数据集"><a href="#2-2-导入数据集" class="headerlink" title="2.2 导入数据集"></a>2.2 导入数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomize</span><span class="params">(dataset, labels)</span>:</span></div><div class="line">    permutation = np.random.permutation(labels.shape[<span class="number">0</span>])</div><div class="line">    shuffed_dataset = dataset[permutation, :, :]</div><div class="line">    shuffed_labels = labels[permutation]</div><div class="line">    <span class="keyword">return</span> shuffed_dataset, shuffed_labels</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot_encode</span><span class="params">(np_array)</span>:</span></div><div class="line">    <span class="keyword">return</span> (np.arange(<span class="number">10</span>) == np_array[:, <span class="keyword">None</span>]).astype(np.float32)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reformat_data</span><span class="params">(dataset, labels, image_width, image_height, image_depth)</span>:</span></div><div class="line">    np_dataset_ = np.array([np.array(image_data).reshape(image_width, image_height, image_depth) <span class="keyword">for</span> image_data <span class="keyword">in</span> dataset])</div><div class="line">    np_labels_ = one_hot_encode(np.array(labels,dtype=np.float32))</div><div class="line">    np_dataset, np_labels = randomize(np_dataset_, np_labels_)</div><div class="line">    <span class="keyword">return</span> np_dataset, np_labels</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_tf_array</span><span class="params">(array)</span>:</span></div><div class="line">    shape = array.get_shape.as_list()</div><div class="line">    <span class="keyword">return</span> tf.reshape(array, [shape[<span class="number">0</span>], shape[<span class="number">1</span>] * shape[<span class="number">2</span>] * shape[<span class="number">3</span>]])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(predictions, labels)</span>:</span></div><div class="line">    <span class="keyword">return</span> (<span class="number">100.0</span> * np.sum(np.argmax(predictions, <span class="number">1</span>) == np.argmax(labels, <span class="number">1</span>)) / predictions.shape[<span class="number">0</span>])</div></pre></td></tr></table></figure>
<ul>
<li>导入数据集</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">%pwd</div></pre></td></tr></table></figure>
<pre><code>&apos;E:\\jupyter notebook&apos;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">################################### 导入 mnist 数据集 ################################</span></div><div class="line"><span class="keyword">from</span> mnist <span class="keyword">import</span> MNIST</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mnist_folder = <span class="string">'.\data\mnist'</span></div><div class="line">mnist_image_width = <span class="number">28</span></div><div class="line">mnist_image_height = <span class="number">28</span></div><div class="line">mnist_image_depth = <span class="number">1</span></div><div class="line">mnist_num_labels = <span class="number">10</span></div><div class="line">mndata = MNIST(mnist_folder)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入训练集和测试集</span></div><div class="line">mnist_train_dataset_, mnist_train_labels_ = mndata.load_training()</div><div class="line">mnist_test_dataset_, mnist_test_labels_ = mndata.load_testing()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 重新改变数据集的形状</span></div><div class="line">mnist_train_dataset, mnist_train_labels = reformat_data(mnist_train_dataset_, mnist_train_labels_, mnist_image_width, mnist_image_height, mnist_image_depth)</div><div class="line">mnist_test_dataset, mnist_test_labels = reformat_data(mnist_test_dataset_, mnist_test_labels_, mnist_image_width, mnist_image_height, mnist_image_depth)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 打印所导入数据集的相关信息</span></div><div class="line">print(<span class="string">"There are &#123;&#125; images, each of size &#123;&#125;."</span>.format(len(mnist_train_dataset), len(mnist_train_dataset[<span class="number">0</span>])))</div><div class="line">print(<span class="string">"Meaning each image has the size of 28*28*1 = &#123;&#125;."</span>.format(mnist_image_width * mnist_image_height * <span class="number">1</span>))</div><div class="line">print(<span class="string">"The training set contains the following &#123;&#125; labels: &#123;&#125;."</span>.format(len(np.unique(mnist_train_labels_)), np.unique(mnist_train_labels_)))</div><div class="line"></div><div class="line">print(<span class="string">"Training set shape "</span>, mnist_train_dataset.shape, mnist_train_labels.shape)</div><div class="line">print(<span class="string">"Test set shape "</span>, mnist_test_dataset.shape, mnist_test_labels.shape)</div></pre></td></tr></table></figure>
<pre><code>There are 60000 images, each of size 28.
Meaning each image has the size of 28*28*1 = 784.
The training set contains the following 10 labels: [0 1 2 3 4 5 6 7 8 9].
Training set shape  (60000, 28, 28, 1) (60000, 10)
Test set shape  (10000, 28, 28, 1) (10000, 10)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 显示图像</span></div><div class="line">im = mnist_train_dataset[<span class="number">0</span>].reshape([<span class="number">28</span>,<span class="number">28</span>])</div><div class="line">image = Image.fromarray(im)</div><div class="line">plt.imshow(image)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://otue1rxl3.bkt.clouddn.com/17-11-27/10952232.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train_dataset_mnist, train_labels_mnist = mnist_train_dataset, mnist_train_labels</div><div class="line">test_dataset_mnist, test_labels_mnist = mnist_test_dataset, mnist_test_labels</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">################################## 导入 CIFAR-10 数据集 ###############################</span></div><div class="line"><span class="keyword">import</span> pickle</div><div class="line"></div><div class="line">cifar10_folder = <span class="string">'.\\data\\cifar10\\'</span></div><div class="line">train_datasets = [<span class="string">'data_batch_1'</span>, <span class="string">'data_batch_2'</span>, <span class="string">'data_batch_3'</span>, <span class="string">'data_batch_4'</span>, <span class="string">'data_batch_5'</span>,]</div><div class="line">test_dataset = [<span class="string">'test_batch'</span>]</div><div class="line">c10_image_height = <span class="number">32</span></div><div class="line">c10_image_width = <span class="number">32</span></div><div class="line">c10_image_depth = <span class="number">3</span></div><div class="line">c10_num_labels = <span class="number">10</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入测试集</span></div><div class="line"><span class="keyword">with</span> open(cifar10_folder + test_dataset[<span class="number">0</span>], <span class="string">'rb'</span>) <span class="keyword">as</span> f0:</div><div class="line">    c10_test_dict = pickle.load(f0, encoding=<span class="string">'bytes'</span>)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">c10_test_dataset, c10_test_labels = c10_test_dict[<span class="string">b'data'</span>], c10_test_dict[<span class="string">b'labels'</span>]</div><div class="line">test_dataset_cifar10, test_labels_cifar10 = reformat_data(c10_test_dataset, c10_test_labels, c10_image_width, c10_image_height, c10_image_depth)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入训练集</span></div><div class="line">c10_train_dataset, c10_train_labels = [], []</div><div class="line"><span class="keyword">for</span> train_dataset <span class="keyword">in</span> train_datasets:</div><div class="line">    <span class="keyword">with</span> open(cifar10_folder + train_dataset, <span class="string">'rb'</span>) <span class="keyword">as</span> f0:</div><div class="line">        c10_train_dict = pickle.load(f0, encoding=<span class="string">'bytes'</span>)</div><div class="line">        c10_train_dataset_, c10_train_labels_ = c10_train_dict[<span class="string">b'data'</span>], c10_train_dict[<span class="string">b'labels'</span>]</div><div class="line">        </div><div class="line">        c10_train_dataset.append(c10_train_dataset_)</div><div class="line">        c10_train_labels += c10_train_labels_</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">c10_train_dataset = np.concatenate(c10_train_dataset, axis=<span class="number">0</span>)</div><div class="line">train_dataset_cifar10, train_labels_cifar10 = reformat_data(c10_train_dataset, c10_train_labels, c10_image_width, c10_image_height, c10_image_depth)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 打印所导入数据集的相关信息</span></div><div class="line">print(<span class="string">"The training set contains the following labels: &#123;&#125;"</span>.format(np.unique(c10_train_dict[<span class="string">b'labels'</span>])))</div><div class="line">print(<span class="string">"Training set shape "</span>, train_dataset_cifar10.shape, train_labels_cifar10.shape)</div><div class="line">print(<span class="string">"Test set shape "</span>, test_dataset_cifar10.shape, test_labels_cifar10.shape)</div></pre></td></tr></table></figure>
<pre><code>The training set contains the following labels: [0 1 2 3 4 5 6 7 8 9]
Training set shape  (50000, 32, 32, 3) (50000, 10)
Test set shape  (10000, 32, 32, 3) (10000, 10)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">c10_im = train_dataset_cifar10[<span class="number">0</span>]</div><div class="line">c10_image_r = Image.fromarray(c10_im[:, :, <span class="number">0</span>]).convert(<span class="string">'L'</span>)</div><div class="line">c10_image_g = Image.fromarray(c10_im[:, :, <span class="number">1</span>]).convert(<span class="string">'L'</span>)</div><div class="line">c10_image_b = Image.fromarray(c10_im[:, :, <span class="number">2</span>]).convert(<span class="string">'L'</span>)</div><div class="line">c10_image = Image.merge(<span class="string">"RGB"</span>, (c10_image_r, c10_image_g, c10_image_b))</div><div class="line">plt.imshow(c10_image)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://otue1rxl3.bkt.clouddn.com/17-11-27/91538923.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">c10_im[:,:,<span class="number">0</span>]</div></pre></td></tr></table></figure>
<pre><code>array([[196, 203, 217, ..., 148, 120,  86],
       [113, 113, 126, ...,  99, 146, 144],
       [127, 116, 159, ..., 190, 203, 174],
       ..., 
       [158, 112,  81, ..., 113,  79,  39],
       [143, 131,  67, ...,  44,  38,  51],
       [123, 125, 111, ...,  53,  79, 120]], dtype=uint8)
</code></pre><h2 id="2-3-搭建一个简单的神经网络"><a href="#2-3-搭建一个简单的神经网络" class="headerlink" title="2.3 搭建一个简单的神经网络"></a>2.3 搭建一个简单的神经网络</h2><p>最简单的神经网络为一层线性全连接神经网络，数学上它由矩阵乘法组成。</p>
<p>在学习TensorFlow的过程中，最好先从这样最简单的神经网络开始，然后再接触更加复杂的神经网络。当我们开始研究更复杂的神经网络时，只有图的模型（步骤二）和权重（步骤三）将会发生改变，而其他部分都保持不变。</p>
<p>下面开始构建一个类似的简单神经网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">image_width = mnist_image_width</div><div class="line">image_height = mnist_image_height</div><div class="line">image_depth = mnist_image_depth</div><div class="line">num_labels = mnist_num_labels</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 数据集</span></div><div class="line">train_dataset = mnist_train_dataset</div><div class="line">train_labels = mnist_train_labels</div><div class="line">test_dataset = mnist_test_dataset</div><div class="line">test_labels = mnist_test_labels</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 迭代次数和学习速率</span></div><div class="line">num_steps = <span class="number">10001</span></div><div class="line">display_step = <span class="number">1000</span></div><div class="line">batch_size = <span class="number">1</span></div><div class="line">learning_rate = <span class="number">0.001</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth))</div><div class="line"><span class="comment"># shape = array.get_shape.as_list()</span></div><div class="line">shape = tf_train_dataset.shape.as_list()</div><div class="line">print(shape)</div><div class="line">tf_train_dataset_reshape = tf.reshape(tf_train_dataset, [shape[<span class="number">0</span>], shape[<span class="number">1</span>] * shape[<span class="number">2</span>] * shape[<span class="number">3</span>]])</div><div class="line">print(tf_train_dataset_reshape.shape)</div></pre></td></tr></table></figure>
<pre><code>[1, 28, 28, 1]
(1, 784)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 构建TensorFlow图</span></div><div class="line">graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> graph.as_default():</div><div class="line">    <span class="comment"># 1) 首先将数据按照TensorFlow友好的格式输入</span></div><div class="line">    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth))</div><div class="line">    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))</div><div class="line">    tf_test_dataset = tf.constant(test_dataset, tf.float32)</div><div class="line">    </div><div class="line">    <span class="comment"># 2) 初始化权重矩阵和偏差向量</span></div><div class="line">    <span class="comment"># 默认使用tf.truncated_normal() 产生权重矩阵，</span></div><div class="line">    <span class="comment"># 默认使用tf.zeros() 产生偏差向量</span></div><div class="line">    weights = tf.Variable(tf.truncated_normal([image_width * image_height * image_depth, num_labels]), tf.float32)</div><div class="line">    bias = tf.Variable(tf.zeros([num_labels]), tf.float32)</div><div class="line">    </div><div class="line">    <span class="comment"># 3) 定义模型</span></div><div class="line">    <span class="comment"># 由矩阵乘法构成的一层全连接神经网络</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(data, weights, bias)</span>:</span></div><div class="line">        shape = data.shape.as_list()</div><div class="line">        data = tf.reshape(data, [shape[<span class="number">0</span>], shape[<span class="number">1</span>] * shape[<span class="number">2</span>] * shape[<span class="number">3</span>]])</div><div class="line">        <span class="keyword">return</span> tf.matmul(data, weights) + bias</div><div class="line">    </div><div class="line">    logits = model(tf_train_dataset, weights, bias)</div><div class="line">    </div><div class="line">    <span class="comment"># 4) 计算loss</span></div><div class="line">    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))</div><div class="line">    </div><div class="line">    <span class="comment"># 5) 选择优化器，进行优化</span></div><div class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</div><div class="line">    </div><div class="line">    <span class="comment"># 6) 训练数据集和测试数据集中图像的预测值分配给变量train_prediction和test_prediction。</span></div><div class="line">    <span class="comment"># It is only necessary if you want to know the accuracy by comparing it with the actual values.</span></div><div class="line">    train_prediction = tf.nn.softmax(logits)</div><div class="line">    test_prediction = tf.nn.softmax(model(tf_test_dataset, weights, bias))</div><div class="line">    </div><div class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</div><div class="line">    tf.global_variables_initializer().run()</div><div class="line">    print(<span class="string">'Initialized with learning_rate'</span>, learning_rate)</div><div class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</div><div class="line">        </div><div class="line">        offset = (step * batch_size) % (train_labels.shape[<span class="number">0</span>] - batch_size)</div><div class="line">        batch_data = train_dataset[offset: (offset + batch_size), :, :, :]</div><div class="line">        batch_labels = train_labels[offset: (offset + batch_size), :]</div><div class="line">        feed_dict = &#123;tf_train_dataset: batch_data, tf_train_labels: batch_labels&#125;</div><div class="line">        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)</div><div class="line">        </div><div class="line">        <span class="keyword">if</span> step % display_step == <span class="number">0</span>:</div><div class="line">            train_accuracy = accuracy(predictions, batch_labels)</div><div class="line">            test_accuracy = accuracy(test_prediction.eval(), test_labels)</div><div class="line">            message = <span class="string">"step &#123;:04d&#125; : loss is &#123;:06.2f&#125;, accuracy on training set &#123;:02.2f&#125; %, accuracy on test set &#123;:02.2f&#125; %"</span>.format(step, l, train_accuracy, test_accuracy)</div><div class="line">            print(message)</div></pre></td></tr></table></figure>
<pre><code>Initialized with learning_rate 0.001
step 0000 : loss is 000.00, accuracy on training set 100.00 %, accuracy on test set 4.30 %
step 1000 : loss is 000.00, accuracy on training set 100.00 %, accuracy on test set 78.88 %
step 2000 : loss is 10744.32, accuracy on training set 0.00 %, accuracy on test set 78.16 %
step 3000 : loss is 000.00, accuracy on training set 100.00 %, accuracy on test set 83.81 %
step 4000 : loss is 000.00, accuracy on training set 100.00 %, accuracy on test set 82.82 %
step 5000 : loss is 000.00, accuracy on training set 100.00 %, accuracy on test set 85.01 %
step 6000 : loss is 000.00, accuracy on training set 100.00 %, accuracy on test set 84.43 %
step 7000 : loss is 000.00, accuracy on training set 100.00 %, accuracy on test set 83.90 %
step 8000 : loss is 000.00, accuracy on training set 100.00 %, accuracy on test set 84.62 %
step 9000 : loss is 000.00, accuracy on training set 100.00 %, accuracy on test set 85.80 %
step 10000 : loss is 000.00, accuracy on training set 100.00 %, accuracy on test set 84.31 %
</code></pre><ul>
<li>上面是原作者的代码，好像有什么问题，目前还太菜了，等懂得多了再来看看；</li>
<li>下面的代码源于机器之心，亲测好使；</li>
<li>下面我们实现的神经网络共有三层，输入层有 784 个神经元，隐藏层与输出层分别有 500 和 10 个神经元。这所以这样设计是因为 MNIST 的像素为28×28=784，所以每一个输入神经元对应于一个灰度像素点。机器之心执行该模型得到的效果非常好，该模型在批量大小为 100，并使用学习率衰减的情况下迭代 10000 步能得到 98.34% 的测试集准确度，以下是该模型代码：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"></div><div class="line"><span class="comment">#加载MNIST数据集</span></div><div class="line">mnist = input_data.read_data_sets(<span class="string">"./data/mnist/"</span>, one_hot=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<pre><code>Extracting ./data/mnist/train-images-idx3-ubyte.gz
Extracting ./data/mnist/train-labels-idx1-ubyte.gz
Extracting ./data/mnist/t10k-images-idx3-ubyte.gz
Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div></pre></td><td class="code"><pre><div class="line">INPUT_NODE = <span class="number">784</span>     </div><div class="line">OUTPUT_NODE = <span class="number">10</span>     </div><div class="line">LAYER1_NODE = <span class="number">500</span>         </div><div class="line">BATCH_SIZE = <span class="number">100</span>       </div><div class="line"></div><div class="line"><span class="comment"># 模型相关的参数</span></div><div class="line">LEARNING_RATE_BASE = <span class="number">0.8</span>      </div><div class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span>    </div><div class="line">REGULARAZTION_RATE = <span class="number">0.0001</span>   </div><div class="line">TRAINING_STEPS = <span class="number">10000</span>        </div><div class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span> </div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, avg_class, weights1, biases1, weights2, biases2)</span>:</span></div><div class="line">    <span class="comment"># 使用滑动平均类</span></div><div class="line">    <span class="keyword">if</span> avg_class == <span class="keyword">None</span>:</div><div class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)</div><div class="line">        <span class="keyword">return</span> tf.matmul(layer1, weights2) + biases2</div><div class="line"></div><div class="line">    <span class="keyword">else</span>:</div><div class="line"></div><div class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))</div><div class="line">        <span class="keyword">return</span> tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)  </div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(mnist)</span>:</span></div><div class="line">    x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, INPUT_NODE], name=<span class="string">'x-input'</span>)</div><div class="line">    y_ = tf.placeholder(tf.float32, [<span class="keyword">None</span>, OUTPUT_NODE], name=<span class="string">'y-input'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 生成隐藏层的参数。</span></div><div class="line">    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=<span class="number">0.1</span>))</div><div class="line">    biases1 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[LAYER1_NODE]))</div><div class="line"></div><div class="line">    <span class="comment"># 生成输出层的参数。</span></div><div class="line">    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=<span class="number">0.1</span>))</div><div class="line">    biases2 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[OUTPUT_NODE]))</div><div class="line"></div><div class="line">    <span class="comment"># 计算不含滑动平均类的前向传播结果</span></div><div class="line">    y = inference(x, <span class="keyword">None</span>, weights1, biases1, weights2, biases2)</div><div class="line"></div><div class="line">    <span class="comment"># 定义训练轮数及相关的滑动平均类 </span></div><div class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</div><div class="line">    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</div><div class="line">    variables_averages_op = variable_averages.apply(tf.trainable_variables())</div><div class="line">    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)</div><div class="line"></div><div class="line">    <span class="comment"># 计算交叉熵及其平均值</span></div><div class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</div><div class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy)</div><div class="line"></div><div class="line">    <span class="comment"># 定义交叉熵损失函数加上正则项为模型损失函数</span></div><div class="line">    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)</div><div class="line">    regularaztion = regularizer(weights1) + regularizer(weights2)</div><div class="line">    loss = cross_entropy_mean + regularaztion</div><div class="line"></div><div class="line">    <span class="comment"># 设置指数衰减的学习率。</span></div><div class="line">    <span class="comment"># 函数返回衰减学习速率</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">    exponential_decay(</span></div><div class="line"><span class="string">        learning_rate,</span></div><div class="line"><span class="string">        global_step,</span></div><div class="line"><span class="string">        decay_steps,</span></div><div class="line"><span class="string">        decay_rate,</span></div><div class="line"><span class="string">        staircase=False,</span></div><div class="line"><span class="string">        name=None</span></div><div class="line"><span class="string">    )</span></div><div class="line"><span class="string">    '''</span></div><div class="line">    <span class="comment"># decayed_learning_rate = learning_rate *decay_rate ^ (global_step / decay_steps)</span></div><div class="line">    learning_rate = tf.train.exponential_decay(</div><div class="line">        LEARNING_RATE_BASE,</div><div class="line">        global_step,</div><div class="line">        mnist.train.num_examples / BATCH_SIZE,</div><div class="line">        LEARNING_RATE_DECAY,</div><div class="line">        staircase=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 随机梯度下降优化器优化损失函数</span></div><div class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</div><div class="line"></div><div class="line">    <span class="comment"># 反向传播更新参数和更新每一个参数的滑动平均值</span></div><div class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, variables_averages_op]):</div><div class="line">        train_op = tf.no_op(name=<span class="string">'train'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 计算准确度</span></div><div class="line">    correct_prediction = tf.equal(tf.argmax(average_y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</div><div class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line"></div><div class="line">    <span class="comment"># 初始化会话并开始训练过程。</span></div><div class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">        tf.global_variables_initializer().run()</div><div class="line">        validate_feed = &#123;x: mnist.validation.images, y_: mnist.validation.labels&#125;</div><div class="line">        test_feed = &#123;x: mnist.test.images, y_: mnist.test.labels&#125; </div><div class="line"></div><div class="line">        <span class="comment"># 循环地训练神经网络。</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</div><div class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</div><div class="line">                validate_acc = sess.run(accuracy, feed_dict=validate_feed)</div><div class="line">                print(<span class="string">"After %d training step(s), validation accuracy using average model is %g "</span> % (i, validate_acc))</div><div class="line"></div><div class="line">            xs,ys=mnist.train.next_batch(BATCH_SIZE)</div><div class="line">            sess.run(train_op,feed_dict=&#123;x:xs,y_:ys&#125;)</div><div class="line"></div><div class="line">        test_acc=sess.run(accuracy,feed_dict=test_feed)</div><div class="line">        print((<span class="string">"After %d training step(s), test accuracy using average model is %g"</span> %(TRAINING_STEPS, test_acc)))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train(mnist)</div></pre></td></tr></table></figure>
<pre><code>After 0 training step(s), validation accuracy using average model is 0.0882 
After 1000 training step(s), validation accuracy using average model is 0.9766 
After 2000 training step(s), validation accuracy using average model is 0.9802 
After 3000 training step(s), validation accuracy using average model is 0.9834 
After 4000 training step(s), validation accuracy using average model is 0.9844 
After 5000 training step(s), validation accuracy using average model is 0.9848 
After 6000 training step(s), validation accuracy using average model is 0.9844 
After 7000 training step(s), validation accuracy using average model is 0.9856 
After 8000 training step(s), validation accuracy using average model is 0.9848 
After 9000 training step(s), validation accuracy using average model is 0.985 
After 10000 training step(s), test accuracy using average model is 0.9838
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mnist.train.num_examples</div><div class="line">mnist.validation.images.shape</div></pre></td></tr></table></figure>
<pre><code>(5000, 784)
</code></pre><h2 id="2-4-多面的TensorFlow"><a href="#2-4-多面的TensorFlow" class="headerlink" title="2.4 多面的TensorFlow"></a>2.4 多面的TensorFlow</h2><p>TensorFlow包含很多不同的函数，这意味着相同的操作可以用不同的函数来完成，举个例子：</p>
<p>logits = tf.matmul(tf_train_dataset, weights) + biases</p>
<p>可以替代为：</p>
<p>logits = tf.nn.xw_plus_b(train_dataset, weights, biases)</p>
<p>使用一个高度集成的函数可以使得构建一个包含多层的神经网络变得简单，这个在TensorFlow的API中有很好的体现。例如：conv_2d()或者fully_connected()函数分别构建了卷积层和全连接层，通过这些函数，层级的数量、滤波器的大小/深度、激活函数的类型等都可以明确地作为一个参数。权重矩阵和偏置向量能自动创建，附加激活函数和 dropout 正则化层同样也能轻松构建。</p>
<p>举例：利用层API定义卷积层网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">w1 = tf.Variable(tf.truncated_normal([filter_size, filter_size, image_depth, filter_depth], stddev=<span class="number">1</span>))</div><div class="line">b1 = tf.Variable(tf.zeros([filter_depth]))</div><div class="line"></div><div class="line">layer1_conv = tf.nn.conv2d(data, w1, [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">layer1_relu = tf.nn.relu(layer1_conv + b1)</div><div class="line">layer1_pool = tf.nn.max_pool(layer1_pool, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div></pre></td></tr></table></figure>
<p>以上的代码可以替换为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tflearn.layers.conv <span class="keyword">import</span> conv_2d, max_pool_2d</div><div class="line"> </div><div class="line">layer1_conv = conv_2d(data, filter_depth, filter_size, activation=<span class="string">'relu'</span>)</div><div class="line">layer1_pool = max_pool_2d(layer1_conv_relu, <span class="number">2</span>, strides=<span class="number">2</span>)</div></pre></td></tr></table></figure>
<p>在替换上述代码之前需要确认是否安装TensorFlow高级API——<a href="https://github.com/tflearn/tflearn" target="_blank" rel="external">TFlearn</a>,TFlearn<a href="http://tflearn.org/installation/" target="_blank" rel="external">官网</a>。</p>
<p>正如我们所见到的，我们不需要定义权重，偏差或者激活函数，尤其是在需要搭建多层神经网络时，这样可以保持代码紧凑干净。</p>
<p>然而，如果我们是刚开始使用TensorFlow，想要学习搭建不同的神经网络，那么使用高级API就不那么合适，因为，这些高级API把所有事情都做完了。</p>
<h2 id="2-5-搭建LeNet5卷积神经网络"><a href="#2-5-搭建LeNet5卷积神经网络" class="headerlink" title="2.5 搭建LeNet5卷积神经网络"></a>2.5 搭建LeNet5卷积神经网络</h2><p>LeNet5 卷积网络架构最早是 Yann LeCun 提出来的，它是早期的一种卷积神经网络，并且可以用来识别手写数字。虽然它在 MNIST 数据集上执行地非常好，但在其它高分辨率和大数据集上性能有所降低。对于这些大数据集，像 AlexNet、VGGNet 或 ResNet 那样的深度卷积网络才执行地十分优秀。</p>
<p>因为 LeNet5 只由 5 层网络，所以它是学习如何构建卷积网络的最佳起点。LeNet5 的架构如下</p>
<center><br><img src="http://otue1rxl3.bkt.clouddn.com/17-11-28/51381705.jpg" alt=""><br></center>

<p>LeNet5包含5层网络：</p>
<ul>
<li>第一层：卷积层，该卷积层使用Sigmoid激活函数，并且在后面带有平均池化层；</li>
<li>第二层：卷积层，该卷积层使用Sigmoid激活函数，并且在后面带有平均池化层；</li>
<li>第三层：全连接层，该全连接层使用Sigmoid激活函数；</li>
<li>第四层：全连接层，该全连接层使用Sigmoid激活函数；</li>
<li>第五层：输出层。</li>
</ul>
<p>这意味着我们需要构建 5 个权重和偏置项矩阵，我们模型的主体大概需要 12 行代码完成（5 个神经网络层级、2 个池化层、4 个激活函数还有 1 个 flatten 层）。因为代码比较多，所以我们最好在计算图之外就定义好独立的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入相关包</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomize</span><span class="params">(dataset, labels)</span>:</span></div><div class="line">    permutation = np.random.permutation(labels.shape[<span class="number">0</span>])</div><div class="line">    shuffed_dataset = dataset[permutation, :, :]</div><div class="line">    shuffed_labels = labels[permutation]</div><div class="line">    <span class="keyword">return</span> shuffed_dataset, shuffed_labels</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot_encode</span><span class="params">(np_array)</span>:</span></div><div class="line">    <span class="keyword">return</span> (np.arange(<span class="number">10</span>) == np_array[:, <span class="keyword">None</span>]).astype(np.float32)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reformat_data</span><span class="params">(dataset, labels, image_width, image_height, image_depth)</span>:</span></div><div class="line">    np_dataset_ = np.array([np.array(image_data).reshape(image_width, image_height, image_depth) <span class="keyword">for</span> image_data <span class="keyword">in</span> dataset])</div><div class="line">    np_labels_ = one_hot_encode(np.array(labels,dtype=np.float32))</div><div class="line">    np_dataset, np_labels = randomize(np_dataset_, np_labels_)</div><div class="line">    <span class="keyword">return</span> np_dataset, np_labels</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_tf_array</span><span class="params">(array)</span>:</span></div><div class="line">    shape = array.get_shape.as_list()</div><div class="line">    <span class="keyword">return</span> tf.reshape(array, [shape[<span class="number">0</span>], shape[<span class="number">1</span>] * shape[<span class="number">2</span>] * shape[<span class="number">3</span>]])</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(predictions, labels)</span>:</span></div><div class="line">    <span class="keyword">return</span> (<span class="number">100.0</span> * np.sum(np.argmax(predictions, <span class="number">1</span>) == np.argmax(labels, <span class="number">1</span>)) / predictions.shape[<span class="number">0</span>])</div></pre></td></tr></table></figure>
<ul>
<li>导入数据集</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">%pwd</div></pre></td></tr></table></figure>
<pre><code>&apos;E:\\jupyter notebook&apos;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">################################### 导入 mnist 数据集 ################################</span></div><div class="line"><span class="keyword">from</span> mnist <span class="keyword">import</span> MNIST</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mnist_folder = <span class="string">'.\data\mnist'</span></div><div class="line">mnist_image_width = <span class="number">28</span></div><div class="line">mnist_image_height = <span class="number">28</span></div><div class="line">mnist_image_depth = <span class="number">1</span></div><div class="line">mnist_num_labels = <span class="number">10</span></div><div class="line">mndata = MNIST(mnist_folder)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入训练集和测试集</span></div><div class="line">mnist_train_dataset_, mnist_train_labels_ = mndata.load_training()</div><div class="line">mnist_test_dataset_, mnist_test_labels_ = mndata.load_testing()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 重新改变数据集的形状</span></div><div class="line">mnist_train_dataset, mnist_train_labels = reformat_data(mnist_train_dataset_, mnist_train_labels_, mnist_image_width, mnist_image_height, mnist_image_depth)</div><div class="line">mnist_test_dataset, mnist_test_labels = reformat_data(mnist_test_dataset_, mnist_test_labels_, mnist_image_width, mnist_image_height, mnist_image_depth)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 打印所导入数据集的相关信息</span></div><div class="line">print(<span class="string">"There are &#123;&#125; images, each of size &#123;&#125;."</span>.format(len(mnist_train_dataset), len(mnist_train_dataset[<span class="number">0</span>])))</div><div class="line">print(<span class="string">"Meaning each image has the size of 28*28*1 = &#123;&#125;."</span>.format(mnist_image_width * mnist_image_height * <span class="number">1</span>))</div><div class="line">print(<span class="string">"The training set contains the following &#123;&#125; labels: &#123;&#125;."</span>.format(len(np.unique(mnist_train_labels_)), np.unique(mnist_train_labels_)))</div><div class="line"></div><div class="line">print(<span class="string">"Training set shape "</span>, mnist_train_dataset.shape, mnist_train_labels.shape)</div><div class="line">print(<span class="string">"Test set shape "</span>, mnist_test_dataset.shape, mnist_test_labels.shape)</div></pre></td></tr></table></figure>
<pre><code>There are 60000 images, each of size 28.
Meaning each image has the size of 28*28*1 = 784.
The training set contains the following 10 labels: [0 1 2 3 4 5 6 7 8 9].
Training set shape  (60000, 28, 28, 1) (60000, 10)
Test set shape  (10000, 28, 28, 1) (10000, 10)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 显示图像</span></div><div class="line">im = mnist_train_dataset[<span class="number">0</span>].reshape([<span class="number">28</span>,<span class="number">28</span>])</div><div class="line">image = Image.fromarray(im)</div><div class="line">plt.imshow(image)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://otue1rxl3.bkt.clouddn.com/17-11-28/39600002.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train_dataset_mnist, train_labels_mnist = mnist_train_dataset, mnist_train_labels</div><div class="line">test_dataset_mnist, test_labels_mnist = mnist_test_dataset, mnist_test_labels</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">LENET5_BATCH_SIZE = <span class="number">32</span></div><div class="line">LENET5_PATCH_SIZE = <span class="number">5</span></div><div class="line">LENET5_PATCH_DEPTH_1 = <span class="number">6</span></div><div class="line">LENET5_PATCH_DEPTH_2 = <span class="number">16</span></div><div class="line">LENET5_NUM_HIDDEN_1 = <span class="number">120</span></div><div class="line">LENET5_NUM_HIDDEN_2 = <span class="number">84</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">variabels_lenet5</span><span class="params">(patch_size = LENET5_PATCH_SIZE,</span></span></div><div class="line"><span class="function"><span class="params">                     patch_depth1 = LENET5_PATCH_DEPTH_1,</span></span></div><div class="line"><span class="function"><span class="params">                     patch_depth2 = LENET5_PATCH_DEPTH_2,</span></span></div><div class="line"><span class="function"><span class="params">                     num_hidden1 = LENET5_NUM_HIDDEN_1,</span></span></div><div class="line"><span class="function"><span class="params">                     num_hidden2 = LENET5_NUM_HIDDEN_2,</span></span></div><div class="line"><span class="function"><span class="params">                     image_depth = <span class="number">1</span>, num_labels = <span class="number">10</span>)</span>:</span></div><div class="line">    w1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, image_depth, patch_depth1], stddev=<span class="number">0.1</span>))</div><div class="line">    b1 = tf.Variable(tf.zeros([patch_depth1]))</div><div class="line"> </div><div class="line">    w2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, patch_depth1, patch_depth2], stddev=<span class="number">0.1</span>))</div><div class="line">    b2 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[patch_depth2]))</div><div class="line">    </div><div class="line">    w3 = tf.Variable(tf.truncated_normal([<span class="number">5</span>*<span class="number">5</span>*patch_depth2, num_hidden1], stddev=<span class="number">0.1</span>))</div><div class="line">    b3 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[num_hidden1]))</div><div class="line">    </div><div class="line">    w4 = tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev=<span class="number">0.1</span>))</div><div class="line">    b4 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[num_hidden2]))</div><div class="line">    </div><div class="line">    w5 = tf.Variable(tf.truncated_normal([num_hidden2, num_labels], stddev=<span class="number">0.1</span>))</div><div class="line">    b5 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[num_labels]))</div><div class="line">    </div><div class="line">    variables = &#123;</div><div class="line">        <span class="string">'w1'</span>:w1, <span class="string">'w2'</span>:w2, <span class="string">'w3'</span>:w3, <span class="string">'w4'</span>:w4, <span class="string">'w5'</span>:w5,</div><div class="line">        <span class="string">'b1'</span>:b1, <span class="string">'b2'</span>:b2, <span class="string">'b3'</span>:b3, <span class="string">'b4'</span>:b4, <span class="string">'b5'</span>:b5</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> variables</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_lenet5</span><span class="params">(data, variables)</span>:</span></div><div class="line">    layer1_conv = tf.nn.conv2d(data, variables[<span class="string">'w1'</span>], [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    layer1_actv = tf.sigmoid(layer1_conv + variables[<span class="string">'b1'</span>])</div><div class="line">    layer1_pool = tf.nn.avg_pool(layer1_actv, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    </div><div class="line">    layer2_conv = tf.nn.conv2d(layer1_pool, variables[<span class="string">'w2'</span>], [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">'VALID'</span>)</div><div class="line">    layer2_actv = tf.sigmoid(layer2_conv + variables[<span class="string">'b2'</span>])</div><div class="line">    layer2_pool = tf.nn.avg_pool(layer2_actv, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># print(layer2_pool.shape.as_list())</span></div><div class="line">    </div><div class="line">    shape = layer2_pool.shape.as_list()</div><div class="line">    layer2_pool = tf.reshape(layer2_pool, [shape[<span class="number">0</span>], shape[<span class="number">1</span>] * shape[<span class="number">2</span>] * shape[<span class="number">3</span>]])</div><div class="line">    flat_layer = layer2_pool</div><div class="line">    layer3_fccd = tf.matmul(flat_layer, variables[<span class="string">'w3'</span>]) + variables[<span class="string">'b3'</span>]</div><div class="line">    layer3_actv = tf.nn.sigmoid(layer3_fccd)</div><div class="line">    </div><div class="line">    layer4_fccd = tf.matmul(layer3_actv, variables[<span class="string">'w4'</span>]) + variables[<span class="string">'b4'</span>]</div><div class="line">    layer4_actv = tf.nn.sigmoid(layer4_fccd)</div><div class="line">    logits = tf.matmul(layer4_actv, variables[<span class="string">'w5'</span>]) + variables[<span class="string">'b5'</span>]</div><div class="line">    <span class="keyword">return</span> logits</div></pre></td></tr></table></figure>
<p>通过上面独立定义的变量和模型，我们可以一点点调整数据流图而不像前面的全连接网络那样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 确定模型尺寸的参数</span></div><div class="line">image_width = mnist_image_width</div><div class="line">image_height = mnist_image_height</div><div class="line">image_depth = mnist_image_depth</div><div class="line">num_labels = mnist_num_labels</div><div class="line"></div><div class="line"><span class="comment"># 数据集</span></div><div class="line">train_dataset = mnist_train_dataset</div><div class="line">train_labels = mnist_train_labels</div><div class="line">test_dataset = mnist_test_dataset</div><div class="line">test_labels = mnist_test_labels</div><div class="line"></div><div class="line"><span class="comment"># 迭代次数和学习速率</span></div><div class="line">num_steps = <span class="number">10001</span></div><div class="line">display_step = <span class="number">1000</span></div><div class="line">learning_rate = <span class="number">0.1</span></div><div class="line">batch_size = <span class="number">64</span></div><div class="line"></div><div class="line">graph = tf.Graph()</div><div class="line"><span class="keyword">with</span> graph.as_default():</div><div class="line">    <span class="comment"># 将数据以TensorFlow友好的形式输入</span></div><div class="line">    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth))</div><div class="line">    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))</div><div class="line">    tf_test_dataset = tf.constant(test_dataset, tf.float32)</div><div class="line">    </div><div class="line">    <span class="comment"># 初始化权重矩阵和偏差向量</span></div><div class="line">    variabels = variabels_lenet5(image_depth=image_depth, num_labels=num_labels)</div><div class="line">    </div><div class="line">    <span class="comment"># 模型用来计算logit</span></div><div class="line">    model = model_lenet5</div><div class="line">    logits = model(tf_train_dataset, variabels)</div><div class="line">    </div><div class="line">    <span class="comment"># 计算softmax交叉熵</span></div><div class="line">    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))</div><div class="line">    </div><div class="line">    <span class="comment"># 梯度下降优化器</span></div><div class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</div><div class="line">    </div><div class="line">    <span class="comment"># 预测</span></div><div class="line">    train_prediction = tf.nn.softmax(logits)</div><div class="line">    test_prediction = tf.nn.softmax(model(tf_test_dataset, variabels))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</div><div class="line">    tf.global_variables_initializer().run()</div><div class="line">    print(<span class="string">'Initialized with learning_rate'</span>, learning_rate)</div><div class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</div><div class="line">        <span class="comment">#Since we are using stochastic gradient descent, we are selecting  small batches from the training dataset,</span></div><div class="line">        <span class="comment">#and training the convolutional neural network each time with a batch.</span></div><div class="line">        offset = (step * batch_size) % (train_labels.shape[<span class="number">0</span>] - batch_size)</div><div class="line">        batch_data = train_dataset[offset: (offset + batch_size), :, :, :]</div><div class="line">        batch_labels = train_labels[offset: (offset + batch_size), :]</div><div class="line">        feed_dict = &#123;tf_train_dataset: batch_data, tf_train_labels: batch_labels&#125;</div><div class="line">        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)</div><div class="line">        </div><div class="line">        <span class="keyword">if</span> step % display_step == <span class="number">0</span>:</div><div class="line">            train_accuracy = accuracy(predictions, batch_labels)</div><div class="line">            test_accuracy = accuracy(test_prediction.eval(), test_labels)</div><div class="line">            message = <span class="string">"step &#123;:04d&#125; : loss is &#123;:06.2f&#125;, accuracy on training set &#123;:02.2f&#125; %, accuracy on test set &#123;:02.2f&#125; %"</span>.format(step, l, train_accuracy, test_accuracy)</div><div class="line">            print(message)</div></pre></td></tr></table></figure>
<pre><code>Initialized with learning_rate 0.1
step 0000 : loss is 002.51, accuracy on training set 15.62 %, accuracy on test set 11.35 %
step 1000 : loss is 002.25, accuracy on training set 32.81 %, accuracy on test set 23.03 %
step 2000 : loss is 000.45, accuracy on training set 89.06 %, accuracy on test set 82.01 %
step 3000 : loss is 000.28, accuracy on training set 96.88 %, accuracy on test set 89.61 %
step 4000 : loss is 000.25, accuracy on training set 95.31 %, accuracy on test set 92.26 %
step 5000 : loss is 000.13, accuracy on training set 96.88 %, accuracy on test set 93.52 %
step 6000 : loss is 000.26, accuracy on training set 90.62 %, accuracy on test set 94.57 %
step 7000 : loss is 000.20, accuracy on training set 95.31 %, accuracy on test set 95.42 %
step 8000 : loss is 000.09, accuracy on training set 98.44 %, accuracy on test set 95.87 %
step 9000 : loss is 000.07, accuracy on training set 100.00 %, accuracy on test set 96.25 %
step 10000 : loss is 000.12, accuracy on training set 95.31 %, accuracy on test set 96.56 %
</code></pre><p>上面是我重现原作者代码所得到的结果，可见程序是没问题的，可以运行，下面是原作者给出的结果,我把batch_size改成64了，可见效果还提升了点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>Initialized <span class="keyword">with</span> learning_rate <span class="number">0.1</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>step <span class="number">0000</span> : loss <span class="keyword">is</span> <span class="number">002.49</span>, accuracy on training set <span class="number">3.12</span> %, accuracy on test set <span class="number">10.09</span> %</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>step <span class="number">1000</span> : loss <span class="keyword">is</span> <span class="number">002.29</span>, accuracy on training set <span class="number">21.88</span> %, accuracy on test set <span class="number">9.58</span> %</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>step <span class="number">2000</span> : loss <span class="keyword">is</span> <span class="number">000.73</span>, accuracy on training set <span class="number">75.00</span> %, accuracy on test set <span class="number">78.20</span> %</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>step <span class="number">3000</span> : loss <span class="keyword">is</span> <span class="number">000.41</span>, accuracy on training set <span class="number">81.25</span> %, accuracy on test set <span class="number">86.87</span> %</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>step <span class="number">4000</span> : loss <span class="keyword">is</span> <span class="number">000.26</span>, accuracy on training set <span class="number">93.75</span> %, accuracy on test set <span class="number">90.49</span> %</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>step <span class="number">5000</span> : loss <span class="keyword">is</span> <span class="number">000.28</span>, accuracy on training set <span class="number">87.50</span> %, accuracy on test set <span class="number">92.79</span> %</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>step <span class="number">6000</span> : loss <span class="keyword">is</span> <span class="number">000.23</span>, accuracy on training set <span class="number">96.88</span> %, accuracy on test set <span class="number">93.64</span> %</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>step <span class="number">7000</span> : loss <span class="keyword">is</span> <span class="number">000.18</span>, accuracy on training set <span class="number">90.62</span> %, accuracy on test set <span class="number">95.14</span> %</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>step <span class="number">8000</span> : loss <span class="keyword">is</span> <span class="number">000.14</span>, accuracy on training set <span class="number">96.88</span> %, accuracy on test set <span class="number">95.80</span> %</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>step <span class="number">9000</span> : loss <span class="keyword">is</span> <span class="number">000.35</span>, accuracy on training set <span class="number">90.62</span> %, accuracy on test set <span class="number">96.33</span> %</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>step <span class="number">10000</span> : loss <span class="keyword">is</span> <span class="number">000.12</span>, accuracy on training set <span class="number">93.75</span> %, accuracy on test set <span class="number">96.76</span> %</div></pre></td></tr></table></figure>
<h2 id="2-6-超参数如何影响一层网络的输出尺寸"><a href="#2-6-超参数如何影响一层网络的输出尺寸" class="headerlink" title="2.6 超参数如何影响一层网络的输出尺寸"></a>2.6 超参数如何影响一层网络的输出尺寸</h2><p>一般来说，确实是层级越多神经网络的性能就越好。我们可以添加更多的层级、更改激活函数和池化层、改变学习率并查看每一步对性能的影响。因为层级 i 的输出是层级 i+1 的输入，所以我们需要知道第 i 层神经网络的超参数如何影响其输出尺寸。</p>
<p>为了理解这一点我们需要讨论一下 conv2d() 函数。</p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">conv2d(</div><div class="line">    input,</div><div class="line">    filter,</div><div class="line">    strides,</div><div class="line">    padding,</div><div class="line">    use_cudnn_on_gpu=<span class="keyword">True</span>,</div><div class="line">    data_format=<span class="string">'NHWC'</span>,</div><div class="line">    name=<span class="keyword">None</span></div><div class="line">)</div></pre></td></tr></table></figure>
<ul>
<li>input: A Tensor. Must be one of the following types: half, float32. A 4-D tensor. The dimension order is interpreted according to the value of data_format, see below for details.</li>
<li>filter: A Tensor. Must have the same type as input. A 4-D tensor of shape [filter_height, filter_width, in_channels, out_channels]</li>
<li>strides: A list of ints. 1-D tensor of length 4. The stride of the sliding window for each dimension of input. The dimension order is determined by the value of data_format, see below for details.</li>
<li>padding: A string from: “SAME”, “VALID”. The type of padding algorithm to use.</li>
<li>use_cudnn_on_gpu: An optional bool. Defaults to True.</li>
<li>data_format: An optional string from: “NHWC”, “NCHW”. Defaults to “NHWC”. Specify the data format of the input and output data. With the -  - default format “NHWC”, the data is stored in the order of: [batch, height, width, channels]. Alternatively, the format could be “NCHW”, the - data storage order of: [batch, channels, height, width].</li>
<li>name: A name for the operation (optional).</li>
</ul>
</blockquote>
<p>该函数有四个参数：</p>
<ul>
<li>输入图像，即一个四维张量 [batch size, image_width, image_height, image_depth]</li>
<li>权重矩阵，即一个四维张量 [filter_size, filter_size, image_depth, filter_depth]</li>
<li>每一个维度的步幅数</li>
<li>Padding (= ‘SAME’ / ‘VALID’)</li>
</ul>
<p>这四个参数决定了输出图像的尺寸。前面两个参数都是四维张量，其包括了批量输入图像的信息和卷积滤波器的权值。</p>
<p>第三个参数为卷积的步幅（stride），即卷积滤波器在 4 个维度中的每一次移动的距离。四个中间的第一个维度代表着图像的批量数，这个维度肯定每次只能移动一张图片。最后一个维度为图片深度（即色彩通道数，1 代表灰度图片，而 3 代表 RGB 图片），因为我们通常并不想跳过任何一个通道，所以这一个值也通常为 1。第二个和第三个维度代表 X 和 Y 方向（图片宽度和高度）的步幅。如果我们希望能应用步幅参数，我们需要设定每个维度的移动步幅。例如设定步幅为 1，那么步幅参数就需要设定为 [1, 1, 1, 1]，如果我们希望在图像上移动的步幅设定为 2，步幅参数为 [1, 2, 2, 1]。</p>
<p>最后一个参数表明 TensorFlow 是否需要使用 0 来填补图像周边，这样以确保图像输出尺寸在步幅参数设定为 1 的情况下保持不变。通过设置 padding = ‘SAME’，图像会只使用 0 来填补周边（输出尺寸不变），而 padding = ‘VALID’则不会使用 0。在下图中，我们将看到两个使用卷积滤波器在图像上扫描的案例，其中滤波器的大小为 5 x 5、图像的大小为 28 x 28。左边的 Padding 参数设置为’SAME’，并且最后四行/列的信息也会包含在输出图像中。而右边 padding 设置为 ‘VALID’，最后四行/列是不包括在输出图像内的。</p>
<center><img src="http://otue1rxl3.bkt.clouddn.com/17-11-28/88148369.jpg" alt=""></center><br>没有 padding 的图片，最后四个像素点是无法包含在内的，因为卷积滤波器已经移动到了图片的边缘。这就意味着输入 28 x 28 尺寸的图片，输出尺寸只有 24 x 24。如果 padding = ‘SAME’，那么输出尺寸就是 28 x 28。<br>如果我们输入图片尺寸是 28 x 28、滤波器尺寸为 5 x 5，步幅分别设置为 1 到 4，那么就能得到下表<br><center><img src="http://otue1rxl3.bkt.clouddn.com/17-11-28/13366246.jpg" alt=""></center><br>由此可见，对于stride为1，用0补充和不用0补充的输出图像大小分别为28x28和24x24,如果stride为2，则为14x14和12x12，如果stride为3，则为10x10和8x8等。<br><br>对于任意给定的步幅 S、滤波器尺寸 K、图像尺寸 W、padding 尺寸 P，输出的图像尺寸可以总结上表的规则如下：<br><br><center>O = 1+(W-K+2P)/S</center>

<h2 id="2-7-调整-LeNet5-架构"><a href="#2-7-调整-LeNet5-架构" class="headerlink" title="2.7 调整 LeNet5 架构"></a>2.7 调整 LeNet5 架构</h2><p>LeNet5 架构在原论文中使用的是 Sigmoid 激活函数和平均池化。然而如今神经网络使用 ReLU 激活函数更为常见。所以我们可以修改一下 LeNet5 架构，并看看是否能获得性能上的提升，我们可以称这种修改的架构为类 LeNet5 架构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line">LENET5_LIKE_BATCH_SIZE = <span class="number">32</span></div><div class="line">LENET5_LIKE_FILTER_SIZE = <span class="number">5</span></div><div class="line">LENET5_LIKE_FILTER_DEPTH = <span class="number">16</span></div><div class="line">LENET5_LIKE_NUM_HIDDEN = <span class="number">120</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">variables_lenet5_like</span><span class="params">(filter_size = LENET5_LIKE_FILTER_SIZE, </span></span></div><div class="line"><span class="function"><span class="params">                          filter_depth = LENET5_LIKE_FILTER_DEPTH, </span></span></div><div class="line"><span class="function"><span class="params">                          num_hidden = LENET5_LIKE_NUM_HIDDEN,</span></span></div><div class="line"><span class="function"><span class="params">                          image_width = <span class="number">28</span>, image_depth = <span class="number">1</span>, num_labels = <span class="number">10</span>)</span>:</span></div><div class="line"> </div><div class="line">    w1 = tf.Variable(tf.truncated_normal([filter_size, filter_size, image_depth, filter_depth], stddev=<span class="number">0.1</span>))</div><div class="line">    b1 = tf.Variable(tf.zeros([filter_depth]))</div><div class="line"></div><div class="line">    w2 = tf.Variable(tf.truncated_normal([filter_size, filter_size, filter_depth, filter_depth], stddev=<span class="number">0.1</span>))</div><div class="line">    b2 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape=[filter_depth]))</div><div class="line"> </div><div class="line">    w3 = tf.Variable(tf.truncated_normal([(image_width // <span class="number">4</span>)*(image_width // <span class="number">4</span>)*filter_depth , num_hidden], stddev=<span class="number">0.1</span>))</div><div class="line">    b3 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape = [num_hidden]))</div><div class="line"></div><div class="line">    w4 = tf.Variable(tf.truncated_normal([num_hidden, num_hidden], stddev=<span class="number">0.1</span>))</div><div class="line">    b4 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape = [num_hidden]))</div><div class="line"> </div><div class="line">    w5 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=<span class="number">0.1</span>))</div><div class="line">    b5 = tf.Variable(tf.constant(<span class="number">1.0</span>, shape = [num_labels]))</div><div class="line">    variables = &#123;</div><div class="line">                  <span class="string">'w1'</span>: w1, <span class="string">'w2'</span>: w2, <span class="string">'w3'</span>: w3, <span class="string">'w4'</span>: w4, <span class="string">'w5'</span>: w5,</div><div class="line">                  <span class="string">'b1'</span>: b1, <span class="string">'b2'</span>: b2, <span class="string">'b3'</span>: b3, <span class="string">'b4'</span>: b4, <span class="string">'b5'</span>: b5</div><div class="line">                &#125;</div><div class="line">    <span class="keyword">return</span> variables</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_lenet5_like</span><span class="params">(data, variables)</span>:</span></div><div class="line">    layer1_conv = tf.nn.conv2d(data, variables[<span class="string">'w1'</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    layer1_actv = tf.nn.relu(layer1_conv + variables[<span class="string">'b1'</span>])</div><div class="line">    layer1_pool = tf.nn.avg_pool(layer1_actv, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line">    layer2_conv = tf.nn.conv2d(layer1_pool, variables[<span class="string">'w2'</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    layer2_actv = tf.nn.relu(layer2_conv + variables[<span class="string">'b2'</span>])</div><div class="line">    layer2_pool = tf.nn.avg_pool(layer2_actv, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">    </div><div class="line">    shape = layer2_pool.shape.as_list()</div><div class="line">    layer2_pool = tf.reshape(layer2_pool, [shape[<span class="number">0</span>], shape[<span class="number">1</span>] * shape[<span class="number">2</span>] * shape[<span class="number">3</span>]])</div><div class="line">    flat_layer = layer2_pool</div><div class="line">    <span class="comment"># flat_layer = flatten_tf_array(layer2_pool)</span></div><div class="line">    layer3_fccd = tf.matmul(flat_layer, variables[<span class="string">'w3'</span>]) + variables[<span class="string">'b3'</span>]</div><div class="line">    layer3_actv = tf.nn.relu(layer3_fccd)</div><div class="line">    <span class="comment">#layer3_drop = tf.nn.dropout(layer3_actv, 0.5)</span></div><div class="line"> </div><div class="line">    layer4_fccd = tf.matmul(layer3_actv, variables[<span class="string">'w4'</span>]) + variables[<span class="string">'b4'</span>]</div><div class="line">    layer4_actv = tf.nn.relu(layer4_fccd)</div><div class="line">   <span class="comment">#layer4_drop = tf.nn.dropout(layer4_actv, 0.5)</span></div><div class="line"> </div><div class="line">    logits = tf.matmul(layer4_actv, variables[<span class="string">'w5'</span>]) + variables[<span class="string">'b5'</span>]</div><div class="line">    <span class="keyword">return</span> logits</div></pre></td></tr></table></figure>
<p>主要区别在于我们使用relu激活函数而不是sigmoid激活。 </p>
<p>除了激活函数之外，我们还可以更改已使用的优化器，以查看不同优化器对精度的影响。</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/tensorflow/" rel="tag"># tensorflow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/10/25/VS2017配置OpenCV3.2.0开发环境/" rel="next" title="Win7下配置VS2017+OpenCV3.2.0">
                <i class="fa fa-chevron-left"></i> Win7下配置VS2017+OpenCV3.2.0
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/12/02/用于具有对准意识的多视角人脸检测的漏斗级联结构/" rel="prev" title="Funnel-Structured Cascade for Multi-View Face Detection with Alignment-Awareness">
                Funnel-Structured Cascade for Multi-View Face Detection with Alignment-Awareness <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="lv-container" data-id="city" data-uid="MTAyMC8yOTc5Ny82MzYz"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="ailee" />
          <p class="site-author-name" itemprop="name">ailee</p>
           
              <p class="site-description motion-element" itemprop="description">优秀不够，一定要卓越，一定要无可替代才是最重要的。</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              友情链接
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="https://rootingc.me" title="rooting" target="_blank">rooting</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://mindthink.me/" title="mindthink" target="_blank">mindthink</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.yoogu.cc/" title="Wakke Wang" target="_blank">Wakke Wang</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://ictar.github.io" title="Ele A面" target="_blank">Ele A面</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-1-张量和图"><span class="nav-number">1.</span> <span class="nav-text">1.1 张量和图</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-2-占位符和feed-dict"><span class="nav-number">2.</span> <span class="nav-text">1.2 占位符和feed_dict</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#下面是使用占位符的案例："><span class="nav-number">2.1.</span> <span class="nav-text">下面是使用占位符的案例：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#解析一段构建了三层全连接神经网络的代码"><span class="nav-number">2.2.</span> <span class="nav-text">解析一段构建了三层全连接神经网络的代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tensorflow中的神经网络"><span class="nav-number">3.</span> <span class="nav-text">tensorflow中的神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-简介"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-导入数据集"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 导入数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-搭建一个简单的神经网络"><span class="nav-number">3.3.</span> <span class="nav-text">2.3 搭建一个简单的神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-多面的TensorFlow"><span class="nav-number">3.4.</span> <span class="nav-text">2.4 多面的TensorFlow</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-搭建LeNet5卷积神经网络"><span class="nav-number">3.5.</span> <span class="nav-text">2.5 搭建LeNet5卷积神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-超参数如何影响一层网络的输出尺寸"><span class="nav-number">3.6.</span> <span class="nav-text">2.6 超参数如何影响一层网络的输出尺寸</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-7-调整-LeNet5-架构"><span class="nav-number">3.7.</span> <span class="nav-text">2.7 调整 LeNet5 架构</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ailee</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      本站访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      本站总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  






  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("3JEOnLBbeCkTnDLXdRB4RQNh-gzGzoHsz", "LpdXae1Pn9iCAuW8OVklkOqp");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  


  

  

</body>
</html>
